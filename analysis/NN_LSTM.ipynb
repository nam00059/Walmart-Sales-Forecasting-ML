{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 04:16:12.889723: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 04:16:13.121136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746245773.141345   13514 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746245773.147471   13514 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746245773.162778   13514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746245773.162797   13514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746245773.162799   13514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746245773.162801   13514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 04:16:13.168077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "#Importing Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={'id'       :     'object', \n",
    "    'item_id'     :  'int64', \n",
    "    'dept_id'     :  'int8', \n",
    "    'cat_id'      :  'int8', \n",
    "    'store_id'    :  'int8', \n",
    "    'state_id'    :  'int8', \n",
    "    'd'           :  'object', \n",
    "    'sales'       :  'int16',  \n",
    "    'date'        : 'object', \n",
    "   'wday'        :  'int8',  \n",
    "   'month'       :  'int8',  \n",
    "   'year'        :  'int16',  \n",
    "   'event_name_1' : 'int8', \n",
    "   'event_type_1' : 'int8', \n",
    "   'event_name_2' : 'int8', \n",
    "   'event_type_2' : 'int8', \n",
    "    'snap':'int8',\n",
    "  'sell_price'   : 'float16',\n",
    "       'price_change':'float16',\n",
    "   'week_number'  : 'int8',  \n",
    "   'season'       : 'object', \n",
    "   'quater_start' : 'int8',  \n",
    "   'quater_end'   : 'int8',  \n",
    "   'month_start'  : 'int8',  \n",
    "   'month_end'    : 'int8',  \n",
    "   'year_start'   : 'int8',  \n",
    "   'year_end'     : 'int8',  \n",
    "   'group'        : 'int8',  \n",
    "   'no_events'    : 'object', \n",
    "   'holiday'      : 'object',\n",
    "    'week_number':'int8',\n",
    "       'season':'int8',\n",
    "       'quater_start':'int8',\n",
    "       'quater_end':'int8',\n",
    "       'month_start':'int8',\n",
    "       'month_end':'int8',\n",
    "       'year_start':'int8',\n",
    "       'year_end':'int8',\n",
    "       'roll_7_shift_28_mean':'float16',\n",
    "       'roll_14_shift_28_mean':'float16',\n",
    "       'roll_30_shift_28_mean':'float16',\n",
    "       'roll_60_shift_28_mean':'float16',\n",
    "       'roll_360_shift_28_mean':'float16',\n",
    "       'roll_7_shift_28_std':'float16',\n",
    "       'roll_14_shift_28_std':'float16',\n",
    "       'roll_30_shift_28_std':'float16',\n",
    "       'roll_60_shift_28_std':'float16',\n",
    "       'roll_360_shift_28_std':'float16',\n",
    "       'direct_ewm':'float16',\n",
    "       'direct_lag_28':'int16',\n",
    "       'direct_lag_35':'int16',\n",
    "       'direct_lag_42':'int16',\n",
    "       'direct_lag_49':'int16',\n",
    "       'direct_lag_56':'int16',\n",
    "       'direct_lag_63':'int16',\n",
    "       'direct_lag_70':'int16',\n",
    "       'direct_lag_77':'int16',\n",
    "       'direct_lag_84':'int16',\n",
    "       'direct_lag_91':'int16',\n",
    "       'direct_lag_98':'int16',\n",
    "       'min_price':'float16',\n",
    "       'max_price':'float16',\n",
    "       'mean_price':'float16',\n",
    "       'std_price':'float16',\n",
    "       'price_norm_1':'float16',\n",
    "       'price_norm_2':'float16',\n",
    "       'price_norm_3':'float16',\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_feather('Intermediate Data/train1.feather')\n",
    "cv=pd.read_feather('Intermediate Data/cv1.feather')\n",
    "test=pd.read_feather('Intermediate Data/test1.feather')\n",
    "final_test=pd.read_feather('Intermediate Data/final_test1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, typ in dtype.items():\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].astype(typ)\n",
    "        cv[col] = cv[col].astype(typ)\n",
    "        test[col] = test[col].astype(typ)\n",
    "        final_test[col] = final_test[col].astype(typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRMSSE Calculation\n",
    "def caluclate_WRMSSE(actual,predicted,train,weights,h,n):\n",
    "    '''This function is used to calculate RMSSE'''\n",
    "    num=((actual-predicted)**2).sum(axis=1)/h\n",
    "    denom=(train[:,1:]-train[:,:-1])**2\n",
    "    denom=denom.sum(axis=1)/(n-1)\n",
    "    return (num/denom)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances(model,train,cv,test,X_cv,X_test):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    cv['pred_sales']=model.predict(X_cv)\n",
    "    df1=cv.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1886,1914)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    cv.drop(['pred_sales'],axis=1,inplace=True)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    test['pred_sales']=model.predict(X_test)\n",
    "    df1=test.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1914,1942)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances_for_store_wise_trained_model(train,cv,test,cv_pred,test_pred):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data where model is trained according to store id(Mainly used for Catabosst)'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    \n",
    "    df1=cv_pred\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    df1=test_pred\n",
    "    df1['id']=df1['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746245968.666501   13514 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input1 = tf.keras.layers.Input(shape=(1,), name='Item_Id')\n",
    "input2 = tf.keras.layers.Input(shape=(1,), name='Dept_Id')\n",
    "input3 = tf.keras.layers.Input(shape=(1,), name='Cat_Id')\n",
    "input4 = tf.keras.layers.Input(shape=(1,), name='Store_Id')\n",
    "input5 = tf.keras.layers.Input(shape=(1,), name='State_Id')\n",
    "input6 = tf.keras.layers.Input(shape=(1,), name='year')\n",
    "input7 = tf.keras.layers.Input(shape=(1,), name='event_name_1')\n",
    "input8 = tf.keras.layers.Input(shape=(1,), name='event_name_2')\n",
    "input9 = tf.keras.layers.Input(shape=(1,), name='season')\n",
    "input10 = tf.keras.layers.Input(shape=(1, 23), name='Numerical_features')\n",
    "\n",
    "emb1=tf.keras.layers.Embedding(3050,output_dim=150)(input1)\n",
    "emb2=tf.keras.layers.Embedding(8,output_dim=10)(input2)\n",
    "emb3=tf.keras.layers.Embedding(4,output_dim=10)(input3)\n",
    "emb4=tf.keras.layers.Embedding(11,output_dim=10)(input4)\n",
    "emb5=tf.keras.layers.Embedding(4,output_dim=10)(input5)\n",
    "emb6=tf.keras.layers.Embedding(2017,output_dim=10)(input6)\n",
    "emb7=tf.keras.layers.Embedding(32,output_dim=10)(input7)\n",
    "emb8=tf.keras.layers.Embedding(6,output_dim=10)(input8)\n",
    "emb9=tf.keras.layers.Embedding(5,output_dim=10)(input9)\n",
    "\n",
    "lstm1=tf.keras.layers.LSTM(50)(emb1)\n",
    "lstm2=tf.keras.layers.LSTM(10)(emb2)\n",
    "lstm3=tf.keras.layers.LSTM(10)(emb3)\n",
    "lstm4=tf.keras.layers.LSTM(10)(emb4)\n",
    "lstm5=tf.keras.layers.LSTM(10)(emb5)\n",
    "lstm6=tf.keras.layers.LSTM(10)(emb6)\n",
    "lstm7=tf.keras.layers.LSTM(10)(emb7)\n",
    "lstm8=tf.keras.layers.LSTM(10)(emb8)\n",
    "lstm9=tf.keras.layers.LSTM(10)(emb9)\n",
    "lstm10=tf.keras.layers.LSTM(10)(input10)\n",
    "\n",
    "x1=tf.keras.layers.Flatten()(lstm1)\n",
    "x2=tf.keras.layers.Flatten()(lstm2)\n",
    "x3=tf.keras.layers.Flatten()(lstm3)\n",
    "x4=tf.keras.layers.Flatten()(lstm4)\n",
    "x5=tf.keras.layers.Flatten()(lstm5)\n",
    "x6=tf.keras.layers.Flatten()(lstm6)\n",
    "x7=tf.keras.layers.Flatten()(lstm7)\n",
    "x8=tf.keras.layers.Flatten()(lstm8)\n",
    "x9=tf.keras.layers.Flatten()(lstm9)\n",
    "x10=tf.keras.layers.Flatten()(input10)\n",
    "\n",
    "x=tf.keras.layers.Concatenate()([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10])\n",
    "x=tf.keras.layers.BatchNormalization()(x)\n",
    "x=tf.keras.layers.Dense(256,activation='sigmoid')(x)\n",
    "x=tf.keras.layers.Dense(128,activation='tanh')(x)\n",
    "x=tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(1)(x)\n",
    "model=tf.keras.Model([input1,input2,input3,input4,input5,input6,input7,input8,input9,input10],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0009),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - loss: 6.8024 - val_loss: 4.8700\n",
      "Epoch 2/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - loss: 6.7789 - val_loss: 4.9005\n",
      "Epoch 3/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - loss: 6.7771 - val_loss: 4.9070\n",
      "Epoch 4/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - loss: 6.7270 - val_loss: 4.9074\n",
      "Epoch 5/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 3s/step - loss: 6.7206 - val_loss: 4.8924\n",
      "Epoch 6/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 4s/step - loss: 6.6850 - val_loss: 4.9062\n",
      "Epoch 7/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 4s/step - loss: 6.7096 - val_loss: 4.8993\n",
      "Epoch 8/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 4s/step - loss: 6.6655 - val_loss: 4.9286\n",
      "Epoch 9/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 4s/step - loss: 6.7662 - val_loss: 4.8954\n",
      "Epoch 10/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 4s/step - loss: 6.6566 - val_loss: 4.9137\n",
      "Epoch 11/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 4s/step - loss: 6.6790 - val_loss: 4.9258\n",
      "Epoch 12/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 4s/step - loss: 6.5915 - val_loss: 4.9126\n",
      "Epoch 13/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 4s/step - loss: 6.6397 - val_loss: 4.8877\n",
      "Epoch 14/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 4s/step - loss: 6.6132 - val_loss: 4.9681\n",
      "Epoch 15/15\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 4s/step - loss: 6.6495 - val_loss: 4.9197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f6b26ce2740>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[\n",
    "        train['item_id'].values.reshape(-1,1),\n",
    "        train['dept_id'].values.reshape(-1,1),\n",
    "        train['cat_id'].values.reshape(-1,1),\n",
    "        train['store_id'].values.reshape(-1,1),\n",
    "        train['state_id'].values.reshape(-1,1),\n",
    "        train['year'].values.reshape(-1,1),\n",
    "        train['event_name_1'].values.reshape(-1,1),\n",
    "        train['event_name_2'].values.reshape(-1,1),\n",
    "        train['season'].values.reshape(-1,1),\n",
    "        train[\n",
    "            [\n",
    "                'roll_7_shift_28_mean', 'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "                'roll_60_shift_28_mean', 'roll_360_shift_28_mean', 'roll_7_shift_28_std',\n",
    "                'roll_14_shift_28_std', 'roll_30_shift_28_std', 'roll_60_shift_28_std',\n",
    "                'roll_360_shift_28_std', 'sell_price', 'direct_lag_28', 'direct_lag_35',\n",
    "                'direct_lag_42', 'direct_lag_49', 'direct_lag_56', 'direct_lag_63',\n",
    "                'direct_lag_70', 'direct_lag_77', 'direct_lag_84', 'direct_lag_91',\n",
    "                'direct_lag_98', 'direct_ewm'\n",
    "            ]\n",
    "        ].values.reshape(-1,1,23)\n",
    "    ],\n",
    "    y=train['sales'],\n",
    "    validation_data=(\n",
    "        [\n",
    "            cv['item_id'].values.reshape(-1,1),\n",
    "            cv['dept_id'].values.reshape(-1,1),\n",
    "            cv['cat_id'].values.reshape(-1,1),\n",
    "            cv['store_id'].values.reshape(-1,1),\n",
    "            cv['state_id'].values.reshape(-1,1),\n",
    "            cv['year'].values.reshape(-1,1),\n",
    "            cv['event_name_1'].values.reshape(-1,1),\n",
    "            cv['event_name_2'].values.reshape(-1,1),\n",
    "            cv['season'].values.reshape(-1,1),\n",
    "            cv[\n",
    "                [\n",
    "                    'roll_7_shift_28_mean', 'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "                    'roll_60_shift_28_mean', 'roll_360_shift_28_mean', 'roll_7_shift_28_std',\n",
    "                    'roll_14_shift_28_std', 'roll_30_shift_28_std', 'roll_60_shift_28_std',\n",
    "                    'roll_360_shift_28_std', 'sell_price', 'direct_lag_28', 'direct_lag_35',\n",
    "                    'direct_lag_42', 'direct_lag_49', 'direct_lag_56', 'direct_lag_63',\n",
    "                    'direct_lag_70', 'direct_lag_77', 'direct_lag_84', 'direct_lag_91',\n",
    "                    'direct_lag_98', 'direct_ewm'\n",
    "                ]\n",
    "            ].values.reshape(-1,1,23)\n",
    "        ],\n",
    "        cv['sales']\n",
    "    ),\n",
    "    batch_size=1000000,\n",
    "    epochs=15,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26679/26679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step\n",
      "\u001b[1m26679/26679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2ms/step\n",
      "CV WRMSSE= 0.9158538873602474\n",
      "Test WRMSSE= 0.8622420347782332\n"
     ]
    }
   ],
   "source": [
    "get_model_performances(model,train,cv,test,[cv['item_id'].values.reshape(-1,1),cv['dept_id'].values,cv['cat_id'].values.reshape(-1,1),cv['store_id'].values.reshape(-1,1),\\\n",
    "             cv['state_id'].values.reshape(-1,1),cv['year'].values.reshape(-1,1),cv['event_name_1'].values.reshape(-1,1),cv['event_name_2'].values.reshape(-1,1),\\\n",
    "             cv['season'].values.reshape(-1,1),cv[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],\\\n",
    "                       [test['item_id'].values.reshape(-1,1),test['dept_id'].values,test['cat_id'].values.reshape(-1,1),test['store_id'].values.reshape(-1,1),\\\n",
    "             test['state_id'].values.reshape(-1,1),test['year'].values.reshape(-1,1),test['event_name_1'].values.reshape(-1,1),test['event_name_2'].values.reshape(-1,1),\\\n",
    "             test['season'].values.reshape(-1,1),test[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_model.keras') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26679/26679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test['pred_sales']=model.predict([test['item_id'].values.reshape(-1,1),test['dept_id'].values,test['cat_id'].values.reshape(-1,1),test['store_id'].values.reshape(-1,1),\\\n",
    "             test['state_id'].values.reshape(-1,1),test['year'].values.reshape(-1,1),test['event_name_1'].values.reshape(-1,1),test['event_name_2'].values.reshape(-1,1),\\\n",
    "             test['season'].values.reshape(-1,1),test[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26679/26679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "final_test['pred_sales']=model.predict([final_test['item_id'].values.reshape(-1,1),test['dept_id'].values,final_test['cat_id'].values.reshape(-1,1),final_test['store_id'].values.reshape(-1,1),\\\n",
    "             final_test['state_id'].values.reshape(-1,1),final_test['year'].values.reshape(-1,1),final_test['event_name_1'].values.reshape(-1,1),final_test['event_name_2'].values.reshape(-1,1),\\\n",
    "             final_test['season'].values.reshape(-1,1),final_test[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test.pivot_table(index='id',columns='date',values='pred_sales')\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df['id']=df['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "dic={}\n",
    "l=test['date'].unique()\n",
    "for i,day in enumerate(l):\n",
    "    dic[day]='F'+str(i+1)\n",
    "df.rename(columns=dic,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=final_test.pivot_table(index='id',columns='d',values='pred_sales')\n",
    "df1.reset_index(level=0,inplace=True)\n",
    "dic={}\n",
    "for i,day in enumerate(range(1942,1970)):\n",
    "    dic['d_'+str(day)]='F'+str(i+1)\n",
    "df1.rename(columns=dic,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('lstm_submission_1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305854,
     "sourceId": 11653920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
