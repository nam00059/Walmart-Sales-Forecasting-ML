{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-02T22:01:23.096061Z",
     "iopub.status.busy": "2025-05-02T22:01:23.095684Z",
     "iopub.status.idle": "2025-05-02T22:01:23.101656Z",
     "shell.execute_reply": "2025-05-02T22:01:23.100508Z",
     "shell.execute_reply.started": "2025-05-02T22:01:23.096027Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import All required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:47:35.779795Z",
     "iopub.status.busy": "2025-05-02T19:47:35.779234Z",
     "iopub.status.idle": "2025-05-02T19:47:51.776227Z",
     "shell.execute_reply": "2025-05-02T19:47:51.775374Z",
     "shell.execute_reply.started": "2025-05-02T19:47:35.779753Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n",
    "df1=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv') #this is used for training\n",
    "df2=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "df3=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv') # this is used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:47:51.777661Z",
     "iopub.status.busy": "2025-05-02T19:47:51.777229Z",
     "iopub.status.idle": "2025-05-02T19:47:51.786187Z",
     "shell.execute_reply": "2025-05-02T19:47:51.784953Z",
     "shell.execute_reply.started": "2025-05-02T19:47:51.777630Z"
    }
   },
   "outputs": [],
   "source": [
    "#Clearly in calender.csv we have many entries which contain NaN in event_type_1,event_type_2,event_name_1 & enent_name_2\n",
    "#We have replaced all those entries with no_event\n",
    "df=df.fillna(value='no_event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:47:51.789539Z",
     "iopub.status.busy": "2025-05-02T19:47:51.789159Z",
     "iopub.status.idle": "2025-05-02T19:48:06.059057Z",
     "shell.execute_reply": "2025-05-02T19:48:06.058019Z",
     "shell.execute_reply.started": "2025-05-02T19:47:51.789513Z"
    }
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(1,1914):\n",
    "  l.append(\"d_\"+str(i))\n",
    "df_final=pd.melt(df1,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                 value_vars=l,var_name=\"d\",value_name=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:48:06.060280Z",
     "iopub.status.busy": "2025-05-02T19:48:06.060003Z",
     "iopub.status.idle": "2025-05-02T19:48:06.168944Z",
     "shell.execute_reply": "2025-05-02T19:48:06.167810Z",
     "shell.execute_reply.started": "2025-05-02T19:48:06.060258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taking only last 28 data days of test bcz eariler values are same as for train\n",
    "l=[]\n",
    "for i in range(1914,1942):\n",
    "  l.append(\"d_\"+str(i))\n",
    "df_final_test=pd.melt(df3,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                 value_vars=l,var_name=\"d\",value_name=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:48:06.170630Z",
     "iopub.status.busy": "2025-05-02T19:48:06.170313Z",
     "iopub.status.idle": "2025-05-02T19:48:06.191658Z",
     "shell.execute_reply": "2025-05-02T19:48:06.190662Z",
     "shell.execute_reply.started": "2025-05-02T19:48:06.170607Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1942,1970):\n",
    "    df3['d_'+str(i)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:48:06.193197Z",
     "iopub.status.busy": "2025-05-02T19:48:06.192803Z",
     "iopub.status.idle": "2025-05-02T19:48:06.336041Z",
     "shell.execute_reply": "2025-05-02T19:48:06.335118Z",
     "shell.execute_reply.started": "2025-05-02T19:48:06.193168Z"
    }
   },
   "outputs": [],
   "source": [
    "#Also create future data to be used for futures sales data\n",
    "l=[]\n",
    "for i in range(1942,1970):\n",
    "    l.append(\"d_\"+str(i))\n",
    "df_future_data=pd.melt(df3,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                 value_vars=l,var_name=\"d\",value_name=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:48:06.337300Z",
     "iopub.status.busy": "2025-05-02T19:48:06.337052Z",
     "iopub.status.idle": "2025-05-02T19:50:30.319592Z",
     "shell.execute_reply": "2025-05-02T19:50:30.318194Z",
     "shell.execute_reply.started": "2025-05-02T19:48:06.337281Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we merge all These 3 dataframes to get final csv file train\n",
    "data=df_final.merge(df,on='d',copy=False)# combine calender.csv and modified trainevaluation.csv on feature 'd'\n",
    "data=data.merge(df2,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False) # combine new dataframe with sell_price.csv usnig features \"store_id\", \"item_id\", \"wm_yr_wk\"\n",
    "data.to_feather('/kaggle/working/final_dataframe.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:50:30.321536Z",
     "iopub.status.busy": "2025-05-02T19:50:30.320989Z",
     "iopub.status.idle": "2025-05-02T19:50:34.942179Z",
     "shell.execute_reply": "2025-05-02T19:50:34.940552Z",
     "shell.execute_reply.started": "2025-05-02T19:50:30.321478Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we merge all These 3 dataframes to get final csv file test\n",
    "data_test=df_final_test.merge(df,on='d',copy=False)# combine calender.csv and modified trainevaluation.csv on feature 'd'\n",
    "data_test=data_test.merge(df2,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False) # combine new dataframe with sell_price.csv usnig features \"store_id\", \"item_id\", \"wm_yr_wk\"\n",
    "data_test.to_feather('/kaggle/working/final_dataframe_test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:50:34.946130Z",
     "iopub.status.busy": "2025-05-02T19:50:34.945777Z",
     "iopub.status.idle": "2025-05-02T19:50:40.113336Z",
     "shell.execute_reply": "2025-05-02T19:50:40.112229Z",
     "shell.execute_reply.started": "2025-05-02T19:50:34.946103Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we merge all These 3 dataframes to get final csv file future data\n",
    "data_future=df_future_data.merge(df,on='d',copy=False)# combine calender.csv and modified trainevaluation.csv on feature 'd'\n",
    "data_future=data_future.merge(df2,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False) # combine new dataframe with sell_price.csv usnig features \"store_id\", \"item_id\", \"wm_yr_wk\"\n",
    "data_future.fillna('no_event',inplace=True)\n",
    "data_future.to_feather('/kaggle/working/final_future_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:50:40.114750Z",
     "iopub.status.busy": "2025-05-02T19:50:40.114361Z",
     "iopub.status.idle": "2025-05-02T19:50:40.120943Z",
     "shell.execute_reply": "2025-05-02T19:50:40.119772Z",
     "shell.execute_reply.started": "2025-05-02T19:50:40.114727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final dataframe train is= (46027957, 22)\n",
      "Shape of final dataframe test is= (853720, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of final dataframe train is=\",data.shape)\n",
    "print(\"Shape of final dataframe test is=\",data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T19:51:15.570436Z",
     "iopub.status.busy": "2025-05-02T19:51:15.570093Z",
     "iopub.status.idle": "2025-05-02T19:51:15.687492Z",
     "shell.execute_reply": "2025-05-02T19:51:15.686438Z",
     "shell.execute_reply.started": "2025-05-02T19:51:15.570388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up RAM to avoid kernel issues\n",
    "del data, data_test, data_future\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading up the dataframes\n",
    "\n",
    "train=pd.read_feather('Intermediate Data/final_dataframe.feather')\n",
    "test=pd.read_feather('Intermediate Data/final_dataframe_test.feather')\n",
    "final_test=pd.read_feather('Intermediate Data/final_future_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Data to Integer to save space in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['item_id']=lbl.fit_transform(train['item_id'])\n",
    "test['item_id']=lbl.transform(test['item_id'])\n",
    "final_test['item_id']=lbl.transform(final_test['item_id'])\n",
    "pickle.dump(lbl,open('label_encoder_item_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['dept_id']=lbl.fit_transform(train['dept_id'])\n",
    "test['dept_id']=lbl.transform(test['dept_id'])\n",
    "final_test['dept_id']=lbl.transform(final_test['dept_id'])\n",
    "pickle.dump(lbl,open('label_encoder_dept_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['cat_id']=lbl.fit_transform(train['cat_id'])\n",
    "test['cat_id']=lbl.transform(test['cat_id'])\n",
    "final_test['cat_id']=lbl.transform(final_test['cat_id'])\n",
    "pickle.dump(lbl,open('label_encoder_cat_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['store_id']=lbl.fit_transform(train['store_id'])\n",
    "test['store_id']=lbl.transform(test['store_id'])\n",
    "final_test['store_id']=lbl.transform(final_test['store_id'])\n",
    "pickle.dump(lbl,open('label_encoder_store_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['state_id']=lbl.fit_transform(train['state_id'])\n",
    "test['state_id']=lbl.transform(test['state_id'])\n",
    "final_test['state_id']=lbl.transform(final_test['state_id'])\n",
    "pickle.dump(lbl,open('label_encoder_state_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_name_1']=lbl.fit_transform(train['event_name_1'])\n",
    "test['event_name_1']=lbl.transform(test['event_name_1'])\n",
    "final_test['event_name_1']=lbl.transform(final_test['event_name_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_name_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_name_2']=lbl.fit_transform(train['event_name_2'])\n",
    "test['event_name_2']=lbl.transform(test['event_name_2'])\n",
    "final_test['event_name_2']=lbl.transform(final_test['event_name_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_name_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_1']=lbl.fit_transform(train['event_type_1'])\n",
    "test['event_type_1']=lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1']=lbl.transform(final_test['event_type_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_2']=lbl.fit_transform(train['event_type_2'])\n",
    "test['event_type_2']=lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2']=lbl.transform(final_test['event_type_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['year']=lbl.fit_transform(train['year'])\n",
    "test['year']=lbl.transform(test['year'])\n",
    "final_test['year']=lbl.transform(final_test['year'])\n",
    "pickle.dump(lbl,open('label_encoder_year.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Unnecessary Columns to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are not using these features\n",
    "cols_to_drop = ['weekday', 'wm_yr_wk']\n",
    "\n",
    "train.drop(columns=cols_to_drop, inplace=True)\n",
    "test.drop(columns=cols_to_drop, inplace=True)\n",
    "final_test.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently collapse snap columns into one\n",
    "train['snap'] = np.where(train['state_id'] == 'CA', train['snap_CA'],\n",
    "                np.where(train['state_id'] == 'TX', train['snap_TX'], train['snap_WI']))\n",
    "\n",
    "# Drop old columns\n",
    "train.drop(['snap_CA','snap_TX','snap_WI'], axis=1, inplace=True)\n",
    "\n",
    "test['snap'] = np.where(test['state_id'] == 'CA', test['snap_CA'],\n",
    "               np.where(test['state_id'] == 'TX', test['snap_TX'], test['snap_WI']))\n",
    "\n",
    "# Drop old columns\n",
    "test.drop(['snap_CA','snap_TX','snap_WI'], axis=1, inplace=True)\n",
    "\n",
    "final_test['snap'] = np.where(final_test['state_id'] == 'CA', final_test['snap_CA'],\n",
    "                    np.where(final_test['state_id'] == 'TX', final_test['snap_TX'], final_test['snap_WI']))\n",
    "\n",
    "# Drop old columns\n",
    "final_test.drop(['snap_CA','snap_TX','snap_WI'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_number(x):\n",
    "    \"\"\"This Function is used to get weeknumber of particular date\"\"\"\n",
    "    date=calendar.datetime.date.fromisoformat(x)\n",
    "    return date.isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['week_number']=train['date'].apply(lambda x:get_week_number(x))\n",
    "test['week_number']=test['date'].apply(lambda x:get_week_number(x))\n",
    "final_test['week_number']=final_test['date'].apply(lambda x:get_week_number(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(x):\n",
    "    \"\"\"This function is used to get season in US according to various months\"\"\"\n",
    "    if x in [12,1,2]:\n",
    "        return 0      #\"Winter\"\n",
    "    elif x in [3,4,5]:\n",
    "        return 1   #\"Spring\"\n",
    "    elif x in [6,7,8]:\n",
    "        return 2   #\"Summer\"\n",
    "    else:\n",
    "        return 3   #\"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['season']=train['month'].apply(lambda x:get_season(x))\n",
    "test['season']=test['month'].apply(lambda x:get_season(x))\n",
    "final_test['season']=final_test['month'].apply(lambda x:get_season(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quater_begin(x):\n",
    "    \"\"\"This is used to check if day is begining of quater\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and (month in [1,4,7,9])) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quater_start']=train['date'].apply(lambda x:check_if_quater_begin(x))\n",
    "test['quater_start']=test['date'].apply(lambda x:check_if_quater_begin(x))\n",
    "final_test['quater_start']=final_test['date'].apply(lambda x:check_if_quater_begin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quater_end(x):\n",
    "    \"\"\"This is used to check if day is end of quater\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    if (day==31 and month==3) or (day==30 and month==6) or (day==30 and month==9) or (day==31 and month==12):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quater_end']=train['date'].apply(lambda x:check_if_quater_end(x))\n",
    "test['quater_end']=test['date'].apply(lambda x:check_if_quater_end(x))\n",
    "final_test['quater_end']=final_test['date'].apply(lambda x:check_if_quater_end(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_start(x):\n",
    "    \"\"\"This is used to check if day is begining of month\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    return 1 if day==1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_start']=train['date'].apply(lambda x:month_start(x))\n",
    "test['month_start']=test['date'].apply(lambda x:month_start(x))\n",
    "final_test['month_start']=final_test['date'].apply(lambda x:month_start(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_end(x):\n",
    "    \"\"\"This is used to check if day is end of month\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    year=calendar.datetime.date.fromisoformat(x).year\n",
    "    leap_yr=(year%4==0) # to check if it is a leap year\n",
    "    val=(day==31 and month==1) or (day==29 if leap_yr else day==28) or (day==31 and month==3) or (day==30 and month==4) or\\\n",
    "        (day==31 and month==5) or (day==30 and month==6) or (day==31 and month==7) or (day==31 and month==8) or\\\n",
    "        (day==30 and month==9) or (day==31 and month==10) or (day==30 and month==11) or (day==31 and month==12)\n",
    "    return 1 if val else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_end']=train['date'].apply(lambda x:month_end(x))\n",
    "test['month_end']=test['date'].apply(lambda x:month_end(x))\n",
    "final_test['month_end']=final_test['date'].apply(lambda x:month_end(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_start(x):\n",
    "    \"\"\"This is used to check if day is begining of year\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and month==1) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_start']=train['date'].apply(lambda x:year_start(x))\n",
    "test['year_start']=test['date'].apply(lambda x:year_start(x))\n",
    "final_test['year_start']=final_test['date'].apply(lambda x:year_start(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_end(x):\n",
    "    \"\"\"This is used to check if day is end of year\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==31 and month==12) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_end']=train['date'].apply(lambda x:year_end(x))\n",
    "test['year_end']=test['date'].apply(lambda x:year_end(x))\n",
    "final_test['year_end']=final_test['date'].apply(lambda x:year_end(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation data will be used for hyperparameter tuning\n",
    "cv=train[train['date']>='2016-03-28']\n",
    "train=train[train['date']<'2016-03-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly we will create these Direct features for train  and CV test and final test data\n",
    "# Code to create one large data for all days\n",
    "gc.collect()\n",
    "tt=pd.concat([train,cv,test,final_test])\n",
    "tt.sort_values(['id','date'],inplace=True)\n",
    "df=tt.pivot_table(index=['item_id','store_id'],columns='date',values='sales')\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/151340958.py:6: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n",
      "  roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std\n"
     ]
    }
   ],
   "source": [
    "#Rolling Features\n",
    "# Here we are taking 28 days shift so as to avoid Data Leakage Problem\n",
    "for aggregate in ['mean','std']:\n",
    "    for shif in [28]:\n",
    "        for r in [7,14,30,60,360]:\n",
    "            roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n",
    "            dates=roll.columns\n",
    "            name=\"roll_\"+str(r)+\"_shift_\"+str(shif)+\"_\"+aggregate\n",
    "            roll=roll.astype('float16')\n",
    "            roll.reset_index(level=[0,1],inplace=True)\n",
    "            roll=pd.melt(roll,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name=name)\n",
    "            roll.fillna(-1,inplace=True)\n",
    "            train=train.merge(roll,on=['item_id','store_id','date'])\n",
    "            cv=cv.merge(roll,on=['item_id','store_id','date'])\n",
    "            final_test=final_test.merge(roll,on=['item_id','store_id','date'])\n",
    "            test=test.merge(roll,on=['item_id','store_id','date'])\n",
    "            print(\"Feature created named :=\",name)\n",
    "            del roll\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/842698858.py:3: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll=df.shift(28,axis=1).ewm(alpha=0.99,axis=1,adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Feature created ewa window of size\n"
     ]
    }
   ],
   "source": [
    "# Adding  Exponential weighted average with shift of 28 days\n",
    "# Shift of 28 days is used to prevent data leakage Problem\n",
    "roll=df.shift(28,axis=1).ewm(alpha=0.99,axis=1,adjust=False).mean()\n",
    "dates=roll.columns\n",
    "roll=roll.astype('float16')\n",
    "roll.reset_index(level=[0,1],inplace=True)\n",
    "roll=pd.melt(roll,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name='direct_ewm')\n",
    "roll.fillna(-1,inplace=True)\n",
    "train=train.merge(roll,on=['item_id','store_id','date'])\n",
    "cv=cv.merge(roll,on=['item_id','store_id','date'])\n",
    "test=test.merge(roll,on=['item_id','store_id','date'])\n",
    "final_test=final_test.merge(roll,on=['item_id','store_id','date'])\n",
    "print(\"Direct Feature created ewa window of size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created for lag 28\n",
      "Feature created for lag 35\n",
      "Feature created for lag 42\n",
      "Feature created for lag 49\n",
      "Feature created for lag 56\n",
      "Feature created for lag 63\n",
      "Feature created for lag 70\n",
      "Feature created for lag 77\n",
      "Feature created for lag 84\n",
      "Feature created for lag 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/58hx6x4d7td4pjmyv14938j40000gn/T/ipykernel_19470/3283151178.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  lag_i.reset_index(level=[0,1],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created for lag 98\n"
     ]
    }
   ],
   "source": [
    "# Now we will also calculate lag features with lag of 28,35,42,49,56,63,70,77,84,91,98 days\n",
    "for lag in range(28,100,7):\n",
    "    i='direct_lag_'+str(lag)\n",
    "    lag_i=df.shift(lag,axis=1)\n",
    "    dates=lag_i.columns\n",
    "    lag_i.reset_index(level=[0,1],inplace=True)\n",
    "    lag_i=pd.melt(lag_i,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name=i)\n",
    "    lag_i.fillna(-1,inplace=True)\n",
    "    lag_i[i]=lag_i[i].astype('int16')\n",
    "    train=train.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    cv=cv.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    test=test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    final_test=final_test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    print(\"Feature created for lag\",lag)\n",
    "    del lag_i\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final feature engineered data\n",
    "train.to_feather('Intermediate Data/train1.feather')\n",
    "cv.to_feather('Intermediate Data/cv1.feather')\n",
    "test.to_feather('Intermediate Data/test1.feather')\n",
    "final_test.to_feather('Intermediate Data/final_test1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final feature engineered data\n",
    "train.to_csv('Intermediate Data/train1.csv')\n",
    "cv.to_csv('Intermediate Data/cv1.csv')\n",
    "test.to_csv('Intermediate Data/test1.csv')\n",
    "final_test.to_csv('Intermediate Data/final_test1.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305854,
     "sourceId": 11653920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
