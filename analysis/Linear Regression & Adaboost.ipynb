{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9f9efc-00cd-4c9f-86ed-cbcae1735027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import dask.dataframe as dk\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e160d8ba-2497-4642-94f0-22a32b5d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this snippet of code is used to reduce memory consumption by dataframe\n",
    "dtype={'id'       :     'object', \n",
    "    'item_id'     :  'int64', \n",
    "    'dept_id'     :  'int8', \n",
    "    'cat_id'      :  'int8', \n",
    "    'store_id'    :  'int8', \n",
    "    'state_id'    :  'int8', \n",
    "    'd'           :  'object', \n",
    "    'sales'       :  'int16',  \n",
    "    'date'        : 'object', \n",
    "   'wday'        :  'int8',  \n",
    "   'month'       :  'int8',  \n",
    "   'year'        :  'int16',  \n",
    "   'event_name_1' : 'int8', \n",
    "   'event_type_1' : 'int8', \n",
    "   'event_name_2' : 'int8', \n",
    "   'event_type_2' : 'int8', \n",
    "    'snap':'int8',\n",
    "  'sell_price'   : 'float16',\n",
    "       'price_change':'float16',\n",
    "   'week_number'  : 'int8',  \n",
    "   'season'       : 'object', \n",
    "   'quater_start' : 'int8',  \n",
    "   'quater_end'   : 'int8',  \n",
    "   'month_start'  : 'int8',  \n",
    "   'month_end'    : 'int8',  \n",
    "   'year_start'   : 'int8',  \n",
    "   'year_end'     : 'int8',  \n",
    "   'group'        : 'int8',  \n",
    "   'no_events'    : 'object', \n",
    "   'holiday'      : 'object',\n",
    "    'week_number':'int8',\n",
    "       'season':'int8',\n",
    "       'quater_start':'int8',\n",
    "       'quater_end':'int8',\n",
    "       'month_start':'int8',\n",
    "       'month_end':'int8',\n",
    "       'year_start':'int8',\n",
    "       'year_end':'int8',\n",
    "       'roll_7_shift_28_mean':'float16',\n",
    "       'roll_14_shift_28_mean':'float16',\n",
    "       'roll_30_shift_28_mean':'float16',\n",
    "       'roll_60_shift_28_mean':'float16',\n",
    "       'roll_360_shift_28_mean':'float16',\n",
    "       'roll_7_shift_28_std':'float16',\n",
    "       'roll_14_shift_28_std':'float16',\n",
    "       'roll_30_shift_28_std':'float16',\n",
    "       'roll_60_shift_28_std':'float16',\n",
    "       'roll_360_shift_28_std':'float16',\n",
    "       'direct_ewm':'float16',\n",
    "       'direct_lag_28':'int16',\n",
    "       'direct_lag_35':'int16',\n",
    "       'direct_lag_42':'int16',\n",
    "       'direct_lag_49':'int16',\n",
    "       'direct_lag_56':'int16',\n",
    "       'direct_lag_63':'int16',\n",
    "       'direct_lag_70':'int16',\n",
    "       'direct_lag_77':'int16',\n",
    "       'direct_lag_84':'int16',\n",
    "       'direct_lag_91':'int16',\n",
    "       'direct_lag_98':'int16',\n",
    "       'min_price':'float16',\n",
    "       'max_price':'float16',\n",
    "       'mean_price':'float16',\n",
    "       'std_price':'float16',\n",
    "       'price_norm_1':'float16',\n",
    "       'price_norm_2':'float16',\n",
    "       'price_norm_3':'float16',\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130adc08-9936-457f-8a55-e11807d16b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_feather('/Users/shreyabharti/Desktop/Predictive Anaytics/Project/feather/train1.feather')\n",
    "cv=pd.read_feather('/Users/shreyabharti/Desktop/Predictive Anaytics/Project/feather/cv1.feather')\n",
    "test=pd.read_feather('/Users/shreyabharti/Desktop/Predictive Anaytics/Project/feather/test1.feather')\n",
    "final_test=pd.read_feather('/Users/shreyabharti/Desktop/Predictive Anaytics/Project/feather/final_test1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1f5c84e-e8c1-4314-bdb2-64317d449063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caluclate_WRMSSE(actual,predicted,train,weights,h,n):\n",
    "    '''This function is used to calculate RMSSE'''\n",
    "    num=((actual-predicted)**2).sum(axis=1)/h\n",
    "    denom=(train[:,1:]-train[:,:-1])**2\n",
    "    denom=denom.sum(axis=1)/(n-1)\n",
    "    return (num/denom)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5333a622-5119-46df-8151-a293a7957f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances(model, train, cv, test, X_cv, X_test):\n",
    "    import gc\n",
    "\n",
    "    # ========== CV WRMSSE ==========\n",
    "    cv['prices'] = cv['sales'] * cv['sell_price']\n",
    "    total_sales = cv.groupby('id').sum()\n",
    "    total = total_sales['sales'].sum()\n",
    "    weight = {i: total_sales.loc[i]['sales'] / total for i in total_sales.index}\n",
    "\n",
    "    train1 = pd.concat([train, cv])\n",
    "    train1.sort_values(['id', 'date'], inplace=True)\n",
    "    train1.fillna(0, inplace=True)\n",
    "\n",
    "    df = train1.pivot_table(index=['id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id'],\n",
    "                            columns='d', values='sales').reset_index()\n",
    "    df.fillna(0, inplace=True)\n",
    "    del train1\n",
    "\n",
    "    cv['pred_sales'] = model.predict(X_cv)\n",
    "    df1 = cv.pivot_table(index='id', columns='d', values='pred_sales')\n",
    "    df1.rename(columns={f'd_{i}': f'F{j+1}' for j, i in enumerate(range(1886, 1914))}, inplace=True)\n",
    "    df1.reset_index(inplace=True)\n",
    "\n",
    "    dd = df.merge(df1, on='id')\n",
    "    dd['weight'] = dd['id'].apply(lambda x: weight[x])\n",
    "    l = [f'd_{i}' for i in range(1, 1914)]\n",
    "    l1 = [f'F{i}' for i in range(1, 29)]\n",
    "\n",
    "    agg_level = {\n",
    "        2: ['state_id'], 3: ['store_id'], 4: ['cat_id'], 5: ['dept_id'],\n",
    "        6: ['state_id', 'cat_id'], 7: ['state_id', 'dept_id'],\n",
    "        8: ['store_id', 'cat_id'], 9: ['store_id', 'dept_id'],\n",
    "        10: ['item_id'], 11: ['item_id', 'state_id']\n",
    "    }\n",
    "\n",
    "    agg = pd.DataFrame(dd[l + l1].sum()).T\n",
    "    agg['weight'] = 1 / 12\n",
    "    agg['level'] = 1\n",
    "    col = agg.columns\n",
    "\n",
    "    for level in agg_level:\n",
    "        temp_df = dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight'] /= 12\n",
    "        temp_df['level'] = level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "\n",
    "    dd['weight'] /= 12\n",
    "    dd['level'] = 12\n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "\n",
    "    actual = agg[[f'd_{i}' for i in range(1886, 1914)]].values\n",
    "    predicted = agg[[f'F{i}' for i in range(1, 29)]].values\n",
    "    training = agg[[f'd_{i}' for i in range(1, 1886)]].values\n",
    "    weights = agg['weight'].values\n",
    "\n",
    "    rmsse = caluclate_WRMSSE(actual, predicted, training, weights, 28, 1886)\n",
    "    cv_wrsmme = np.sum(rmsse * weights)\n",
    "\n",
    "    cv.drop(['pred_sales'], axis=1, inplace=True)\n",
    "    del actual, predicted, training, weights, agg\n",
    "\n",
    "    # ========== Test WRMSSE ==========\n",
    "    test['prices'] = test['sales'] * test['sell_price']\n",
    "    total_sales = test.groupby('id').sum()\n",
    "    total = total_sales['sales'].sum()\n",
    "    weight = {i.replace('evaluation', 'validation'): total_sales.loc[i]['sales'] / total for i in total_sales.index}\n",
    "\n",
    "    test['id'] = test['id'].str.replace('evaluation', 'validation')\n",
    "    train1 = pd.concat([train, cv, test])\n",
    "    train1.sort_values(['id', 'date'], inplace=True)\n",
    "    train1.fillna(0, inplace=True)\n",
    "\n",
    "    df = train1.pivot_table(index=['id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id'],\n",
    "                            columns='d', values='sales').reset_index()\n",
    "    df.fillna(0, inplace=True)\n",
    "    del train1\n",
    "\n",
    "    test['pred_sales'] = model.predict(X_test)\n",
    "    df1 = test.pivot_table(index='id', columns='d', values='pred_sales')\n",
    "    df1.rename(columns={f'd_{i}': f'F{j+1}' for j, i in enumerate(range(1914, 1942))}, inplace=True)\n",
    "    df1.reset_index(inplace=True)\n",
    "\n",
    "    dd = df.merge(df1, on='id')\n",
    "    dd['weight'] = dd['id'].apply(lambda x: weight[x])\n",
    "    l = [f'd_{i}' for i in range(1, 1942)]\n",
    "    l1 = [f'F{i}' for i in range(1, 29)]\n",
    "\n",
    "    agg = pd.DataFrame(dd[l + l1].sum()).T\n",
    "    agg['weight'] = 1 / 12\n",
    "    agg['level'] = 1\n",
    "    col = agg.columns\n",
    "\n",
    "    for level in agg_level:\n",
    "        temp_df = dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight'] /= 12\n",
    "        temp_df['level'] = level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "\n",
    "    dd['weight'] /= 12\n",
    "    dd['level'] = 12\n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "\n",
    "    actual = agg[[f'd_{i}' for i in range(1914, 1942)]].values\n",
    "    predicted = agg[[f'F{i}' for i in range(1, 29)]].values\n",
    "    training = agg[[f'd_{i}' for i in range(1, 1914)]].values\n",
    "    weights = agg['weight'].values\n",
    "\n",
    "    rmsse = caluclate_WRMSSE(actual, predicted, training, weights, 28, 1914)\n",
    "    test_wrsmme = np.sum(rmsse * weights)\n",
    "\n",
    "    print(\"CV WRMSSE =\", cv_wrsmme)\n",
    "    print(\"Test WRMSSE =\", test_wrsmme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1257a94c-4ae5-41fc-8594-2b2a15070bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.15.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl (252.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.7/252.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.4/670.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp312-cp312-macosx_11_0_arm64.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.6/342.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.2 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.0.9 opt-einsum-3.4.0 optree-0.15.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23887ccc-7554-4116-8e0b-96eb46060a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Snippet to prepare data for serveral Models in which we pass sparse matrix\n",
    "#converting item_id into string so that it can be used by Count Vectorizer\n",
    "train['item_id']=train['item_id'].apply(lambda x:str(x))\n",
    "cv['item_id']=cv['item_id'].apply(lambda x:str(x))\n",
    "test['item_id']=test['item_id'].apply(lambda x:str(x))\n",
    "final_test['item_id']=final_test['item_id'].apply(lambda x:str(x))\n",
    "\n",
    "\n",
    "vec_item_id=CountVectorizer(binary=True)\n",
    "train_id=vec_item_id.fit_transform(train['item_id'])\n",
    "train_season=csr_matrix(tf.keras.utils.to_categorical(train['season'],4))\n",
    "train_month=csr_matrix(tf.keras.utils.to_categorical(train['month'],13))\n",
    "train_wday=csr_matrix(tf.keras.utils.to_categorical(train['wday'],8))\n",
    "train_snap=csr_matrix(tf.keras.utils.to_categorical(train['snap'],2))\n",
    "train_dept=csr_matrix(tf.keras.utils.to_categorical(train['dept_id'],7))\n",
    "train_en1=csr_matrix(tf.keras.utils.to_categorical(train['event_name_1'],31))\n",
    "train_en2=csr_matrix(tf.keras.utils.to_categorical(train['event_name_2'],5))\n",
    "train_et1=csr_matrix(tf.keras.utils.to_categorical(train['event_type_1'],5))\n",
    "train_et2=csr_matrix(tf.keras.utils.to_categorical(train['event_type_2'],3))\n",
    "train_yr=csr_matrix(tf.keras.utils.to_categorical(train['year'],6))\n",
    "train_yr_st=csr_matrix(tf.keras.utils.to_categorical(train['year_start'],2))\n",
    "train_yr_end=csr_matrix(tf.keras.utils.to_categorical(train['year_end'],2))\n",
    "train_qt_st=csr_matrix(tf.keras.utils.to_categorical(train['quater_start'],2))\n",
    "train_qt_end=csr_matrix(tf.keras.utils.to_categorical(train['quater_end'],2))\n",
    "train_month_st=csr_matrix(tf.keras.utils.to_categorical(train['month_start'],2))\n",
    "train_month_end=csr_matrix(tf.keras.utils.to_categorical(train['month_end'],2))\n",
    "\n",
    "cv_id=vec_item_id.transform(cv['item_id'])\n",
    "cv_season=csr_matrix(tf.keras.utils.to_categorical(cv['season'],4))\n",
    "cv_month=csr_matrix(tf.keras.utils.to_categorical(cv['month'],13))\n",
    "cv_wday=csr_matrix(tf.keras.utils.to_categorical(cv['wday'],8))\n",
    "cv_snap=csr_matrix(tf.keras.utils.to_categorical(cv['snap'],2))\n",
    "cv_dept=csr_matrix(tf.keras.utils.to_categorical(cv['dept_id'],7))\n",
    "cv_en1=csr_matrix(tf.keras.utils.to_categorical(cv['event_name_1'],31))\n",
    "cv_en2=csr_matrix(tf.keras.utils.to_categorical(cv['event_name_2'],5))\n",
    "cv_et1=csr_matrix(tf.keras.utils.to_categorical(cv['event_type_1'],5))\n",
    "cv_et2=csr_matrix(tf.keras.utils.to_categorical(cv['event_type_2'],3))\n",
    "cv_yr=csr_matrix(tf.keras.utils.to_categorical(cv['year'],6))\n",
    "cv_yr_st=csr_matrix(tf.keras.utils.to_categorical(cv['year_start'],2))\n",
    "cv_yr_end=csr_matrix(tf.keras.utils.to_categorical(cv['year_end'],2))\n",
    "cv_qt_st=csr_matrix(tf.keras.utils.to_categorical(cv['quater_start'],2))\n",
    "cv_qt_end=csr_matrix(tf.keras.utils.to_categorical(cv['quater_end'],2))\n",
    "cv_month_st=csr_matrix(tf.keras.utils.to_categorical(cv['month_start'],2))\n",
    "cv_month_end=csr_matrix(tf.keras.utils.to_categorical(cv['month_end'],2))\n",
    "\n",
    "\n",
    "test_id=vec_item_id.transform(test['item_id'])\n",
    "test_season=csr_matrix(tf.keras.utils.to_categorical(test['season'],4))\n",
    "test_month=csr_matrix(tf.keras.utils.to_categorical(test['month'],13))\n",
    "test_wday=csr_matrix(tf.keras.utils.to_categorical(test['wday'],8))\n",
    "test_snap=csr_matrix(tf.keras.utils.to_categorical(test['snap'],2))\n",
    "test_dept=csr_matrix(tf.keras.utils.to_categorical(test['dept_id'],7))\n",
    "test_en1=csr_matrix(tf.keras.utils.to_categorical(test['event_name_1'],31))\n",
    "test_en2=csr_matrix(tf.keras.utils.to_categorical(test['event_name_2'],5))\n",
    "test_et1=csr_matrix(tf.keras.utils.to_categorical(test['event_type_1'],5))\n",
    "test_et2=csr_matrix(tf.keras.utils.to_categorical(test['event_type_2'],3))\n",
    "test_yr=csr_matrix(tf.keras.utils.to_categorical(test['year'],6))\n",
    "test_yr_st=csr_matrix(tf.keras.utils.to_categorical(test['year_start'],2))\n",
    "test_yr_end=csr_matrix(tf.keras.utils.to_categorical(test['year_end'],2))\n",
    "test_qt_st=csr_matrix(tf.keras.utils.to_categorical(test['quater_start'],2))\n",
    "test_qt_end=csr_matrix(tf.keras.utils.to_categorical(test['quater_end'],2))\n",
    "test_month_st=csr_matrix(tf.keras.utils.to_categorical(test['month_start'],2))\n",
    "test_month_end=csr_matrix(tf.keras.utils.to_categorical(test['month_end'],2))\n",
    "\n",
    "\n",
    "final_test_id=vec_item_id.transform(final_test['item_id'])\n",
    "final_test_season=csr_matrix(tf.keras.utils.to_categorical(final_test['season'],4))\n",
    "final_test_month=csr_matrix(tf.keras.utils.to_categorical(final_test['month'],13))\n",
    "final_test_wday=csr_matrix(tf.keras.utils.to_categorical(final_test['wday'],8))\n",
    "final_test_snap=csr_matrix(tf.keras.utils.to_categorical(final_test['snap'],2))\n",
    "final_test_dept=csr_matrix(tf.keras.utils.to_categorical(final_test['dept_id'],7))\n",
    "final_test_en1=csr_matrix(tf.keras.utils.to_categorical(final_test['event_name_1'],31))\n",
    "final_test_en2=csr_matrix(tf.keras.utils.to_categorical(final_test['event_name_2'],5))\n",
    "final_test_et1=csr_matrix(tf.keras.utils.to_categorical(final_test['event_type_1'],5))\n",
    "final_test_et2=csr_matrix(tf.keras.utils.to_categorical(final_test['event_type_2'],3))\n",
    "final_test_yr=csr_matrix(tf.keras.utils.to_categorical(final_test['year'],6))\n",
    "final_test_yr_st=csr_matrix(tf.keras.utils.to_categorical(final_test['year_start'],2))\n",
    "final_test_yr_end=csr_matrix(tf.keras.utils.to_categorical(final_test['year_end'],2))\n",
    "final_test_qt_st=csr_matrix(tf.keras.utils.to_categorical(final_test['quater_start'],2))\n",
    "final_test_qt_end=csr_matrix(tf.keras.utils.to_categorical(final_test['quater_end'],2))\n",
    "final_test_month_st=csr_matrix(tf.keras.utils.to_categorical(final_test['month_start'],2))\n",
    "final_test_month_end=csr_matrix(tf.keras.utils.to_categorical(final_test['month_end'],2))\n",
    "\n",
    "X_train=hstack((train_id,train_dept,train_month,train_wday,train_yr,\\\n",
    "                train_snap,train_en1,train_en2,train_et1,train_et2,train_yr_st,train_yr_end,\n",
    "                train_qt_st,train_qt_end,train_month_st,train_month_end,\\\n",
    "                train['direct_lag_28'].values.reshape(-1,1),\n",
    "                train['direct_lag_35'].values.reshape(-1,1),train['direct_lag_42'].values.reshape(-1,1),\\\n",
    "                train['direct_lag_49'].values.reshape(-1,1),train['direct_lag_56'].values.reshape(-1,1),\\\n",
    "                train['direct_lag_63'].values.reshape(-1,1),train['direct_lag_70'].values.reshape(-1,1),\\\n",
    "                train['direct_lag_77'].values.reshape(-1,1),train['direct_lag_84'].values.reshape(-1,1),\\\n",
    "                train['direct_lag_91'].values.reshape(-1,1),train['direct_lag_98'].values.reshape(-1,1),\\\n",
    "                train['sell_price'].values.reshape(-1,1))).tocsr()\n",
    "y_train=train['sales'].values\n",
    "\n",
    "X_cv=hstack((cv_id,cv_dept,cv_month,cv_wday,cv_yr,\\\n",
    "                cv_snap,cv_en1,cv_en2,cv_et1,cv_et2,cv_yr_st,cv_yr_end,\n",
    "                cv_qt_st,cv_qt_end,cv_month_st,cv_month_end,\\\n",
    "             cv['direct_lag_28'].values.reshape(-1,1),\n",
    "                cv['direct_lag_35'].values.reshape(-1,1),cv['direct_lag_42'].values.reshape(-1,1),\\\n",
    "                cv['direct_lag_49'].values.reshape(-1,1),cv['direct_lag_56'].values.reshape(-1,1),\\\n",
    "                cv['direct_lag_63'].values.reshape(-1,1),cv['direct_lag_70'].values.reshape(-1,1),\\\n",
    "                cv['direct_lag_77'].values.reshape(-1,1),cv['direct_lag_84'].values.reshape(-1,1),\\\n",
    "                cv['direct_lag_91'].values.reshape(-1,1),cv['direct_lag_98'].values.reshape(-1,1),\\\n",
    "                cv['sell_price'].values.reshape(-1,1)\n",
    "                )).tocsr()\n",
    "y_cv=cv['sales'].values\n",
    "\n",
    "\n",
    "X_test=hstack((test_id,test_dept,test_month,test_wday,test_yr,\\\n",
    "                test_snap,test_en1,test_en2,test_et1,test_et2,test_yr_st,test_yr_end,\n",
    "                test_qt_st,test_qt_end,test_month_st,test_month_end,\\\n",
    "                test['direct_lag_28'].values.reshape(-1,1),\n",
    "                test['direct_lag_35'].values.reshape(-1,1),test['direct_lag_42'].values.reshape(-1,1),\\\n",
    "                test['direct_lag_49'].values.reshape(-1,1),test['direct_lag_56'].values.reshape(-1,1),\\\n",
    "                test['direct_lag_63'].values.reshape(-1,1),test['direct_lag_70'].values.reshape(-1,1),\\\n",
    "                test['direct_lag_77'].values.reshape(-1,1),test['direct_lag_84'].values.reshape(-1,1),\\\n",
    "                test['direct_lag_91'].values.reshape(-1,1),test['direct_lag_98'].values.reshape(-1,1),\\\n",
    "                test['sell_price'].values.reshape(-1,1))).tocsr()\n",
    "y_test=test['sales'].values\n",
    "\n",
    "X_final_test=hstack((final_test_id,final_test_dept,final_test_month,final_test_wday,final_test_yr,\\\n",
    "                final_test_snap,final_test_en1,final_test_en2,final_test_et1,final_test_et2,final_test_yr_st,final_test_yr_end,\n",
    "                final_test_qt_st,final_test_qt_end,final_test_month_st,final_test_month_end,\\\n",
    "               final_test['direct_lag_28'].values.reshape(-1,1),\n",
    "                final_test['direct_lag_35'].values.reshape(-1,1),final_test['direct_lag_42'].values.reshape(-1,1),\\\n",
    "                final_test['direct_lag_49'].values.reshape(-1,1),final_test['direct_lag_56'].values.reshape(-1,1),\\\n",
    "                final_test['direct_lag_63'].values.reshape(-1,1),final_test['direct_lag_70'].values.reshape(-1,1),\\\n",
    "                final_test['direct_lag_77'].values.reshape(-1,1),final_test['direct_lag_84'].values.reshape(-1,1),\\\n",
    "                final_test['direct_lag_91'].values.reshape(-1,1),final_test['direct_lag_98'].values.reshape(-1,1),\\\n",
    "                final_test['sell_price'].values.reshape(-1,1))).tocsr()\n",
    "y_final_test=final_test['sales'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09228318-259b-44d4-9f97-3a07dbb09880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model\n",
      "===============\n",
      "Train RMSE = 2.8463633756840157  CV RMSE = 2.214157611164102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "reg = LinearRegression(n_jobs=7)  # ← removed normalize\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred_tr = reg.predict(X_train)\n",
    "y_pred_cv = reg.predict(X_cv)\n",
    "\n",
    "error_tr = np.sqrt(mean_squared_error(y_train, y_pred_tr))\n",
    "error_tst = np.sqrt(mean_squared_error(y_cv, y_pred_cv))\n",
    "\n",
    "print(\"Linear Regression model\")\n",
    "print(\"=\" * 15)\n",
    "print(\"Train RMSE =\", error_tr, \" CV RMSE =\", error_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86eb897c-062f-4a1b-b5b8-c4a7ad3f5a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV WRMSSE = 0.7767322972180203\n",
      "Test WRMSSE = 1.1099952527131942\n"
     ]
    }
   ],
   "source": [
    "get_model_performances(reg,train,cv,test,X_cv,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db0071d0-95f9-4c46-babb-ddc05e0b317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_sales']=reg.predict(X_test)\n",
    "final_test['pred_sales']=reg.predict(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f344365-467f-48f7-8fde-9e682340e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = make_pipeline(StandardScaler(), LinearRegression(n_jobs=7))\n",
    "#reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "694e7bd7-2e7b-40b7-9fd4-4ebfa2705ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test.pivot_table(index='id',columns='date',values='pred_sales')\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df['id']=df['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "dic={}\n",
    "l=test['date'].unique()\n",
    "for i,day in enumerate(l):\n",
    "    dic[day]='F'+str(i+1)\n",
    "df.rename(columns=dic,inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e44dc7fa-0945-44bd-b274-b77bbd1427c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=final_test.pivot_table(index='id',columns='d',values='pred_sales')\n",
    "df1.reset_index(level=0,inplace=True)\n",
    "dic={}\n",
    "for i,day in enumerate(range(1942,1970)):\n",
    "    dic['d_'+str(day)]='F'+str(i+1)\n",
    "df1.rename(columns=dic,inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41b8387a-7362-458e-a1f9-626b7f92cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9a0a140-f083-4508-b602-035cdab2418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('linear_regression.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f06e3a8c-6be0-4b2e-815f-bd51241b38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Regression model\n",
      "===============\n",
      "Train RMSE= 3.2091373069033975  CV RMSE= 2.551286303614295\n"
     ]
    }
   ],
   "source": [
    "reg=AdaBoostRegressor(n_estimators=5,learning_rate=0.1,loss='square')\n",
    "reg.fit(X_train,y_train)\n",
    "print(\"Adaboost Regression model\")\n",
    "y_pred_tr=reg.predict(X_train)\n",
    "y_pred_cv=reg.predict(X_cv)\n",
    "error_tr=np.sqrt(mean_squared_error(y_train,y_pred_tr))\n",
    "error_tst=np.sqrt(mean_squared_error(y_cv,y_pred_cv))\n",
    "print(\"=\"*15)\n",
    "print(\"Train RMSE=\",error_tr,\" CV RMSE=\",error_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b972b39-3c78-4fb8-b9d7-3028e44a7179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV WRMSSE = 1.1304881971688396\n",
      "Test WRMSSE = 1.1795962492552754\n"
     ]
    }
   ],
   "source": [
    "#Get Test and CV RMSSE\n",
    "get_model_performances(reg,train,cv,test,X_cv,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "422142a2-b25e-4ef0-b54c-02a9013a15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV WRMSSE = 1.1304881971688396\n",
      "Test WRMSSE = 1.1795962492552754\n"
     ]
    }
   ],
   "source": [
    "get_model_performances(reg,train,cv,test,X_cv,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d96a5075-c561-42b3-ace1-8be878e81e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_sales']=reg.predict(X_test)\n",
    "final_test['pred_sales']=reg.predict(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "866a1ec5-48cd-4123-900d-10a05f9f5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test.pivot_table(index='id',columns='date',values='pred_sales')\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df['id']=df['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "dic={}\n",
    "l=test['date'].unique()\n",
    "for i,day in enumerate(l):\n",
    "    dic[day]='F'+str(i+1)\n",
    "df.rename(columns=dic,inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3429c6e-e4d9-4b41-8b31-7ed745d4dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=final_test.pivot_table(index='id',columns='d',values='pred_sales')\n",
    "df1.reset_index(level=0,inplace=True)\n",
    "dic={}\n",
    "for i,day in enumerate(range(1942,1970)):\n",
    "    dic['d_'+str(day)]='F'+str(i+1)\n",
    "df1.rename(columns=dic,inplace=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3eca4f74-f9e1-4f8f-85b5-a0347142bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7a5c569-e51a-41f5-840e-32bdb8bc521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('adaboost.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e1ac0-e820-41e5-ae96-0a0a5f331a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
