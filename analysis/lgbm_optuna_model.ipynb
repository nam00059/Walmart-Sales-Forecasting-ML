{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={'id'       :     'object', \n",
    "    'item_id'     :  'int64', \n",
    "    'dept_id'     :  'int8', \n",
    "    'cat_id'      :  'int8', \n",
    "    'store_id'    :  'int8', \n",
    "    'state_id'    :  'int8', \n",
    "    'd'           :  'object', \n",
    "    'sales'       :  'int16',  \n",
    "    'date'        : 'object', \n",
    "   'wday'        :  'int8',  \n",
    "   'month'       :  'int8',  \n",
    "   'year'        :  'int16',  \n",
    "   'event_name_1' : 'int8', \n",
    "   'event_type_1' : 'int8', \n",
    "   'event_name_2' : 'int8', \n",
    "   'event_type_2' : 'int8', \n",
    "    'snap':'int8',\n",
    "  'sell_price'   : 'float16',\n",
    "       'price_change':'float16',\n",
    "   'week_number'  : 'int8',  \n",
    "   'season'       : 'object', \n",
    "   'quater_start' : 'int8',  \n",
    "   'quater_end'   : 'int8',  \n",
    "   'month_start'  : 'int8',  \n",
    "   'month_end'    : 'int8',  \n",
    "   'year_start'   : 'int8',  \n",
    "   'year_end'     : 'int8',  \n",
    "   'group'        : 'int8',  \n",
    "   'no_events'    : 'object', \n",
    "   'holiday'      : 'object',\n",
    "    'week_number':'int8',\n",
    "       'season':'int8',\n",
    "       'quater_start':'int8',\n",
    "       'quater_end':'int8',\n",
    "       'month_start':'int8',\n",
    "       'month_end':'int8',\n",
    "       'year_start':'int8',\n",
    "       'year_end':'int8',\n",
    "       'roll_7_shift_28_mean':'float16',\n",
    "       'roll_14_shift_28_mean':'float16',\n",
    "       'roll_30_shift_28_mean':'float16',\n",
    "       'roll_60_shift_28_mean':'float16',\n",
    "       'roll_360_shift_28_mean':'float16',\n",
    "       'roll_7_shift_28_std':'float16',\n",
    "       'roll_14_shift_28_std':'float16',\n",
    "       'roll_30_shift_28_std':'float16',\n",
    "       'roll_60_shift_28_std':'float16',\n",
    "       'roll_360_shift_28_std':'float16',\n",
    "       'direct_ewm':'float16',\n",
    "       'direct_lag_28':'int16',\n",
    "       'direct_lag_35':'int16',\n",
    "       'direct_lag_42':'int16',\n",
    "       'direct_lag_49':'int16',\n",
    "       'direct_lag_56':'int16',\n",
    "       'direct_lag_63':'int16',\n",
    "       'direct_lag_70':'int16',\n",
    "       'direct_lag_77':'int16',\n",
    "       'direct_lag_84':'int16',\n",
    "       'direct_lag_91':'int16',\n",
    "       'direct_lag_98':'int16',\n",
    "       'min_price':'float16',\n",
    "       'max_price':'float16',\n",
    "       'mean_price':'float16',\n",
    "       'std_price':'float16',\n",
    "       'price_norm_1':'float16',\n",
    "       'price_norm_2':'float16',\n",
    "       'price_norm_3':'float16',\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_feather('Intermediate Data/train1.feather')\n",
    "cv=pd.read_feather('Intermediate Data/cv1.feather')\n",
    "test=pd.read_feather('Intermediate Data/test1.feather')\n",
    "final_test=pd.read_feather('Intermediate Data/final_test1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, typ in dtype.items():\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].astype(typ)\n",
    "        cv[col] = cv[col].astype(typ)\n",
    "        test[col] = test[col].astype(typ)\n",
    "        final_test[col] = final_test[col].astype(typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRMSSE Calculation\n",
    "def caluclate_WRMSSE(actual,predicted,train,weights,h,n):\n",
    "    '''This function is used to calculate RMSSE'''\n",
    "    num=((actual-predicted)**2).sum(axis=1)/h\n",
    "    denom=(train[:,1:]-train[:,:-1])**2\n",
    "    denom=denom.sum(axis=1)/(n-1)\n",
    "    return (num/denom)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances(model,train,cv,test,X_cv,X_test):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    X_cv = cv[m_lgb.feature_name()]\n",
    "    cv['pred_sales']=model.predict(X_cv)\n",
    "    df1=cv.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1886,1914)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    cv.drop(['pred_sales'],axis=1,inplace=True)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    test['pred_sales']=model.predict(X_test)\n",
    "    df1=test.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1914,1942)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances_for_store_wise_trained_model(train,cv,test,cv_pred,test_pred):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data where model is trained according to store id(Mainly used for Catabosst)'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    \n",
    "    df1=cv_pred\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    df1=test_pred\n",
    "    df1['id']=df1['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical Data's\n",
    "columns=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','wday', 'month', 'year',\n",
    "         'event_name_1','event_type_1', 'event_name_2', 'event_type_2','snap','week_number',\n",
    "         'quater_start', 'quater_end', 'month_start','month_end', 'year_start',\n",
    "         'year_end','season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=lgb.Dataset(train.drop(['id','sales','date','d'],axis=1),label=train['sales'],categorical_feature=columns,free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv=lgb.Dataset(cv.drop(['id','sales','date','d'],axis=1),label=cv['sales'],categorical_feature=columns,free_raw_data=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"tweedie\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"tweedie_variance_power\": trial.suggest_float(\"tweedie_variance_power\", 1.05, 1.3),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 128, 512),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 256, 2047),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 0.95),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 0.95),\n",
    "        \"bagging_freq\": 1,\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 1.0),\n",
    "        \"boost_from_average\": True,\n",
    "        \"max_bin\": 127,\n",
    "        \"bin_construct_sample_cnt\": 20000000,\n",
    "        \"force_row_wise\": True,\n",
    "        \"verbosity\": 1,\n",
    "        \"nthread\": 8,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    # Sample training subset for speed (2M rows)\n",
    "    sample_train = train.iloc[:2_000_000]\n",
    "    X_train = sample_train.drop(['id', 'sales', 'date', 'd'], axis=1)\n",
    "    y_train = sample_train['sales']\n",
    "    \n",
    "    X_valid = cv.drop(['id', 'sales', 'date', 'd'], axis=1)\n",
    "    y_valid = cv['sales']\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=columns, free_raw_data=False)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=columns, free_raw_data=False)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=3000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    rmse = mean_squared_error(y_valid, preds, squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:44:52,920] A new study created in memory with name: no-name-f380042f-b37b-4750-afd5-bbfff9655b50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.90497\tvalid's rmse: 2.73165\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's rmse: 3.18316\tvalid's rmse: 2.70152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:45:03,781] Trial 0 finished with value: 2.7015166741092274 and parameters: {'tweedie_variance_power': 1.2905498889152502, 'learning_rate': 0.04787241406623165, 'num_leaves': 161, 'min_data_in_leaf': 990, 'feature_fraction': 0.7158920962306441, 'bagging_fraction': 0.9394593381415659, 'lambda_l2': 0.19066062854224186}. Best is trial 0 with value: 2.7015166741092274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.28945\tvalid's rmse: 2.72557\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttrain's rmse: 3.14749\tvalid's rmse: 2.70466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:45:27,584] Trial 1 finished with value: 2.7046574303384543 and parameters: {'tweedie_variance_power': 1.2846491908512974, 'learning_rate': 0.024334161002437883, 'num_leaves': 336, 'min_data_in_leaf': 1869, 'feature_fraction': 0.7980324720168311, 'bagging_fraction': 0.7578760944875959, 'lambda_l2': 0.8863543365354135}. Best is trial 0 with value: 2.7015166741092274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.89694\tvalid's rmse: 2.7293\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttrain's rmse: 3.05058\tvalid's rmse: 2.70479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:45:42,101] Trial 2 finished with value: 2.7047854051855222 and parameters: {'tweedie_variance_power': 1.0587351285441886, 'learning_rate': 0.03878217476814893, 'num_leaves': 283, 'min_data_in_leaf': 1342, 'feature_fraction': 0.774439002117032, 'bagging_fraction': 0.6282238175193898, 'lambda_l2': 0.2804984160131688}. Best is trial 0 with value: 2.7015166741092274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.07502\tvalid's rmse: 2.72216\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's rmse: 3.2485\tvalid's rmse: 2.70571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:46:00,237] Trial 3 finished with value: 2.7057065491248573 and parameters: {'tweedie_variance_power': 1.2810128785210904, 'learning_rate': 0.03682927476505364, 'num_leaves': 405, 'min_data_in_leaf': 2032, 'feature_fraction': 0.7500192093977038, 'bagging_fraction': 0.8370033726519526, 'lambda_l2': 0.3190881854217418}. Best is trial 0 with value: 2.7015166741092274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.31321\tvalid's rmse: 2.68205\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttrain's rmse: 3.14049\tvalid's rmse: 2.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:46:21,047] Trial 4 finished with value: 2.669504285157168 and parameters: {'tweedie_variance_power': 1.2281142960221723, 'learning_rate': 0.022923373543951672, 'num_leaves': 144, 'min_data_in_leaf': 1587, 'feature_fraction': 0.6177968624148347, 'bagging_fraction': 0.8365321782322894, 'lambda_l2': 0.5249141234978292}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.7998\tvalid's rmse: 2.7295\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's rmse: 3.03496\tvalid's rmse: 2.71226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:46:35,516] Trial 5 finished with value: 2.712257564375757 and parameters: {'tweedie_variance_power': 1.0641084750530319, 'learning_rate': 0.03891584824994803, 'num_leaves': 319, 'min_data_in_leaf': 662, 'feature_fraction': 0.6486442602339088, 'bagging_fraction': 0.7374786701547289, 'lambda_l2': 0.3709434794681986}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.89935\tvalid's rmse: 2.75999\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttrain's rmse: 2.94171\tvalid's rmse: 2.75867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:46:58,997] Trial 6 finished with value: 2.7586744372890974 and parameters: {'tweedie_variance_power': 1.1619378438313506, 'learning_rate': 0.030975756177335917, 'num_leaves': 483, 'min_data_in_leaf': 701, 'feature_fraction': 0.8102825847146243, 'bagging_fraction': 0.747963343378223, 'lambda_l2': 0.2986929931322625}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.05641\tvalid's rmse: 2.68625\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttrain's rmse: 3.21172\tvalid's rmse: 2.67056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:47:12,304] Trial 7 finished with value: 2.6705560450438957 and parameters: {'tweedie_variance_power': 1.2784312261655546, 'learning_rate': 0.03822961532452176, 'num_leaves': 149, 'min_data_in_leaf': 1816, 'feature_fraction': 0.920719763056264, 'bagging_fraction': 0.638958176888, 'lambda_l2': 0.4022863459733905}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.75472\tvalid's rmse: 2.75473\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's rmse: 3.04911\tvalid's rmse: 2.73135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:47:28,269] Trial 8 finished with value: 2.7313542514691926 and parameters: {'tweedie_variance_power': 1.2311371220130742, 'learning_rate': 0.046847296140623586, 'num_leaves': 381, 'min_data_in_leaf': 424, 'feature_fraction': 0.6573160134706467, 'bagging_fraction': 0.8458162643764391, 'lambda_l2': 0.06101035928593679}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.86937\tvalid's rmse: 2.74954\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's rmse: 3.04802\tvalid's rmse: 2.73899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:47:46,816] Trial 9 finished with value: 2.738986078032088 and parameters: {'tweedie_variance_power': 1.1818844527465175, 'learning_rate': 0.035213707358961004, 'num_leaves': 364, 'min_data_in_leaf': 704, 'feature_fraction': 0.7286711261260523, 'bagging_fraction': 0.88890559373564, 'lambda_l2': 0.8416405050860668}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.47182\tvalid's rmse: 2.75951\n",
      "[200]\ttrain's rmse: 3.10182\tvalid's rmse: 2.70248\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttrain's rmse: 3.15521\tvalid's rmse: 2.69607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:48:16,292] Trial 10 finished with value: 2.6960703663268397 and parameters: {'tweedie_variance_power': 1.1454493399678998, 'learning_rate': 0.015561187448015469, 'num_leaves': 207, 'min_data_in_leaf': 1431, 'feature_fraction': 0.6116085425365668, 'bagging_fraction': 0.6945849402659244, 'lambda_l2': 0.6418054427129801}. Best is trial 4 with value: 2.669504285157168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.35228\tvalid's rmse: 2.70379\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's rmse: 3.17553\tvalid's rmse: 2.66233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:48:36,955] Trial 11 finished with value: 2.6623317866719907 and parameters: {'tweedie_variance_power': 1.2285889964379586, 'learning_rate': 0.020784483750645753, 'num_leaves': 134, 'min_data_in_leaf': 1704, 'feature_fraction': 0.9391226234049175, 'bagging_fraction': 0.6073908237997343, 'lambda_l2': 0.5664066633099015}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.31177\tvalid's rmse: 2.7473\n",
      "[200]\ttrain's rmse: 2.94303\tvalid's rmse: 2.71811\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttrain's rmse: 3.05511\tvalid's rmse: 2.70321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:49:09,801] Trial 12 finished with value: 2.703206262175778 and parameters: {'tweedie_variance_power': 1.2139436218869217, 'learning_rate': 0.01974824431817305, 'num_leaves': 232, 'min_data_in_leaf': 1609, 'feature_fraction': 0.9370085123590249, 'bagging_fraction': 0.8211287957726142, 'lambda_l2': 0.6102465997968168}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.68237\tvalid's rmse: 2.87028\n",
      "[200]\ttrain's rmse: 3.16173\tvalid's rmse: 2.68679\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttrain's rmse: 3.11662\tvalid's rmse: 2.68351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:49:43,998] Trial 13 finished with value: 2.68351095755481 and parameters: {'tweedie_variance_power': 1.2269088265729302, 'learning_rate': 0.01311831439724377, 'num_leaves': 129, 'min_data_in_leaf': 1118, 'feature_fraction': 0.8668237228793166, 'bagging_fraction': 0.685153091712083, 'lambda_l2': 0.568388900795103}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.12283\tvalid's rmse: 2.72325\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttrain's rmse: 3.04988\tvalid's rmse: 2.71609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:50:08,899] Trial 14 finished with value: 2.7160923837688853 and parameters: {'tweedie_variance_power': 1.1294939363473784, 'learning_rate': 0.024497652891041624, 'num_leaves': 221, 'min_data_in_leaf': 1602, 'feature_fraction': 0.8886331617496714, 'bagging_fraction': 0.8082011279853776, 'lambda_l2': 0.7365383851124268}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.24777\tvalid's rmse: 2.72547\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttrain's rmse: 3.10937\tvalid's rmse: 2.69727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:50:31,901] Trial 15 finished with value: 2.6972722114618026 and parameters: {'tweedie_variance_power': 1.2442940294907285, 'learning_rate': 0.022527226331078436, 'num_leaves': 177, 'min_data_in_leaf': 1352, 'feature_fraction': 0.8404608273698941, 'bagging_fraction': 0.9073637218855242, 'lambda_l2': 0.47388853978382406}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.16867\tvalid's rmse: 2.70581\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 3.15743\tvalid's rmse: 2.70463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:50:54,007] Trial 16 finished with value: 2.704625204805068 and parameters: {'tweedie_variance_power': 1.1908987457521285, 'learning_rate': 0.02819529932919895, 'num_leaves': 264, 'min_data_in_leaf': 1664, 'feature_fraction': 0.6887047112168869, 'bagging_fraction': 0.6054349878601735, 'lambda_l2': 0.7200001076855895}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.95551\tvalid's rmse: 2.96365\n",
      "[200]\ttrain's rmse: 3.38543\tvalid's rmse: 2.72305\n",
      "[300]\ttrain's rmse: 3.17015\tvalid's rmse: 2.69791\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttrain's rmse: 3.23465\tvalid's rmse: 2.69159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:51:36,735] Trial 17 finished with value: 2.691592724638547 and parameters: {'tweedie_variance_power': 1.252722831819063, 'learning_rate': 0.010750842535073276, 'num_leaves': 192, 'min_data_in_leaf': 1979, 'feature_fraction': 0.6301918094386262, 'bagging_fraction': 0.7871061632374243, 'lambda_l2': 0.4911795275960197}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.33843\tvalid's rmse: 2.73253\n",
      "[200]\ttrain's rmse: 3.02603\tvalid's rmse: 2.69362\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttrain's rmse: 3.10582\tvalid's rmse: 2.68753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:52:00,732] Trial 18 finished with value: 2.687525063036606 and parameters: {'tweedie_variance_power': 1.1032614848455926, 'learning_rate': 0.017705894728366898, 'num_leaves': 129, 'min_data_in_leaf': 1727, 'feature_fraction': 0.6877087875669501, 'bagging_fraction': 0.6984373538506424, 'lambda_l2': 0.9822180554846027}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.02435\tvalid's rmse: 2.71867\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 3.01294\tvalid's rmse: 2.71833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:52:23,536] Trial 19 finished with value: 2.71832665588652 and parameters: {'tweedie_variance_power': 1.2071625530544152, 'learning_rate': 0.03057882509186615, 'num_leaves': 256, 'min_data_in_leaf': 1141, 'feature_fraction': 0.8426110980037165, 'bagging_fraction': 0.8725288128960691, 'lambda_l2': 0.7187028081486555}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.33438\tvalid's rmse: 2.76102\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttrain's rmse: 3.11189\tvalid's rmse: 2.72462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:53:01,217] Trial 20 finished with value: 2.7246197864516155 and parameters: {'tweedie_variance_power': 1.248254984935879, 'learning_rate': 0.020769432365357023, 'num_leaves': 455, 'min_data_in_leaf': 1480, 'feature_fraction': 0.8974414148369686, 'bagging_fraction': 0.6583428934512645, 'lambda_l2': 0.12324933311750641}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.01691\tvalid's rmse: 2.68433\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's rmse: 3.14784\tvalid's rmse: 2.66294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:53:16,851] Trial 21 finished with value: 2.662941738752399 and parameters: {'tweedie_variance_power': 1.2636841380775574, 'learning_rate': 0.042410362954978166, 'num_leaves': 153, 'min_data_in_leaf': 1815, 'feature_fraction': 0.9494829721571113, 'bagging_fraction': 0.6052282577314139, 'lambda_l2': 0.43115912183605765}. Best is trial 11 with value: 2.6623317866719907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.00588\tvalid's rmse: 2.68957\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's rmse: 3.18937\tvalid's rmse: 2.65691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:53:32,596] Trial 22 finished with value: 2.6569092036433752 and parameters: {'tweedie_variance_power': 1.2673523103588624, 'learning_rate': 0.04354430673290601, 'num_leaves': 173, 'min_data_in_leaf': 1850, 'feature_fraction': 0.9423101826128839, 'bagging_fraction': 0.6032825336431096, 'lambda_l2': 0.5156291098146304}. Best is trial 22 with value: 2.6569092036433752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.01036\tvalid's rmse: 2.68219\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's rmse: 3.14724\tvalid's rmse: 2.65581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:53:49,574] Trial 23 finished with value: 2.6558062009486374 and parameters: {'tweedie_variance_power': 1.266270348465285, 'learning_rate': 0.0430731599135092, 'num_leaves': 185, 'min_data_in_leaf': 1864, 'feature_fraction': 0.9491050260230353, 'bagging_fraction': 0.6026167445247476, 'lambda_l2': 0.420769507026491}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.00138\tvalid's rmse: 2.69102\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttrain's rmse: 3.14087\tvalid's rmse: 2.66095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:54:06,814] Trial 24 finished with value: 2.660954611703378 and parameters: {'tweedie_variance_power': 1.2649078499305906, 'learning_rate': 0.04346654117512989, 'num_leaves': 193, 'min_data_in_leaf': 1920, 'feature_fraction': 0.9143200272169937, 'bagging_fraction': 0.6585242263835346, 'lambda_l2': 0.6469968141554167}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 3.02337\tvalid's rmse: 2.69313\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's rmse: 3.15069\tvalid's rmse: 2.66661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:54:23,861] Trial 25 finished with value: 2.6666117715543556 and parameters: {'tweedie_variance_power': 1.296672456153044, 'learning_rate': 0.04354494685910586, 'num_leaves': 193, 'min_data_in_leaf': 1934, 'feature_fraction': 0.9071686657872586, 'bagging_fraction': 0.6512871650883059, 'lambda_l2': 0.6750345424945405}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.95047\tvalid's rmse: 2.72876\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttrain's rmse: 3.19252\tvalid's rmse: 2.6684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:54:39,979] Trial 26 finished with value: 2.668396769546157 and parameters: {'tweedie_variance_power': 1.2629351839923162, 'learning_rate': 0.04963935171766647, 'num_leaves': 238, 'min_data_in_leaf': 2024, 'feature_fraction': 0.8711791464452602, 'bagging_fraction': 0.6727671517763595, 'lambda_l2': 0.7807464269502227}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.99253\tvalid's rmse: 2.70596\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's rmse: 3.16738\tvalid's rmse: 2.66855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:54:59,088] Trial 27 finished with value: 2.668554183750474 and parameters: {'tweedie_variance_power': 1.2682664257984304, 'learning_rate': 0.04371580535250708, 'num_leaves': 288, 'min_data_in_leaf': 1802, 'feature_fraction': 0.9156733350245699, 'bagging_fraction': 0.6308936002180039, 'lambda_l2': 0.22530003867926113}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.99049\tvalid's rmse: 2.70991\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttrain's rmse: 3.05256\tvalid's rmse: 2.70376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:55:16,832] Trial 28 finished with value: 2.7037631837119425 and parameters: {'tweedie_variance_power': 1.2081551937707495, 'learning_rate': 0.033723925578004375, 'num_leaves': 175, 'min_data_in_leaf': 929, 'feature_fraction': 0.8731711552617695, 'bagging_fraction': 0.7262594810430304, 'lambda_l2': 0.45283213439127645}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 4377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.580102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.90606\tvalid's rmse: 2.74122\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttrain's rmse: 3.08706\tvalid's rmse: 2.70602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:55:34,449] Trial 29 finished with value: 2.70602307955554 and parameters: {'tweedie_variance_power': 1.291133077659551, 'learning_rate': 0.04601260436505606, 'num_leaves': 210, 'min_data_in_leaf': 1505, 'feature_fraction': 0.8407937160040513, 'bagging_fraction': 0.9408037633033202, 'lambda_l2': 0.554790626776351}. Best is trial 23 with value: 2.6558062009486374.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"tweedie\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boost_from_average\": True,\n",
    "    \"max_bin\": 127,\n",
    "    \"bin_construct_sample_cnt\": 20000000,\n",
    "    \"force_row_wise\": True,\n",
    "    \"verbosity\": 1,\n",
    "    \"nthread\": 8,\n",
    "    \"seed\": 42\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 6049\n",
      "[LightGBM] [Info] Number of data points in the train set: 45174237, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.356321\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's rmse: 2.68306\tvalid's rmse: 2.1763\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttrain's rmse: 2.68306\tvalid's rmse: 2.1763\n"
     ]
    }
   ],
   "source": [
    "m_lgb = lgb.train(\n",
    "    best_params,\n",
    "    data,\n",
    "    valid_sets=[data, data_cv],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)  # same as verbose_eval=1\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x31529e860>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lgb.save_model('lgbm_optuna_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV WRMSSE= 0.6402907900201394\n",
      "Test WRMSSE= 0.7770103022417408\n"
     ]
    }
   ],
   "source": [
    "get_model_performances(m_lgb,train,cv,test,cv.drop(['id','sales','date','d'],axis=1),test.drop(['id','sales','date','d'],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_sales']=m_lgb.predict(test.drop(['id','sales','date','d','prices', 'pred_sales'],axis=1))\n",
    "final_test['pred_sales']=m_lgb.predict(final_test.drop(['id','sales','date','d',],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test.pivot_table(index='id',columns='date',values='pred_sales')\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df['id']=df['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "dic={}\n",
    "l=test['date'].unique()\n",
    "for i,day in enumerate(l):\n",
    "    dic[day]='F'+str(i+1)\n",
    "df.rename(columns=dic,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=final_test.pivot_table(index='id',columns='d',values='pred_sales')\n",
    "df1.reset_index(level=0,inplace=True)\n",
    "dic={}\n",
    "for i,day in enumerate(range(1942,1970)):\n",
    "    dic['d_'+str(day)]='F'+str(i+1)\n",
    "df1.rename(columns=dic,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('lgbm_submission_2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305854,
     "sourceId": 11653920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
