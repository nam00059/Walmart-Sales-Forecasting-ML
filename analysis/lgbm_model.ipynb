{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={'id'       :     'object', \n",
    "    'item_id'     :  'int64', \n",
    "    'dept_id'     :  'int8', \n",
    "    'cat_id'      :  'int8', \n",
    "    'store_id'    :  'int8', \n",
    "    'state_id'    :  'int8', \n",
    "    'd'           :  'object', \n",
    "    'sales'       :  'int16',  \n",
    "    'date'        : 'object', \n",
    "   'wday'        :  'int8',  \n",
    "   'month'       :  'int8',  \n",
    "   'year'        :  'int16',  \n",
    "   'event_name_1' : 'int8', \n",
    "   'event_type_1' : 'int8', \n",
    "   'event_name_2' : 'int8', \n",
    "   'event_type_2' : 'int8', \n",
    "    'snap':'int8',\n",
    "  'sell_price'   : 'float16',\n",
    "       'price_change':'float16',\n",
    "   'week_number'  : 'int8',  \n",
    "   'season'       : 'object', \n",
    "   'quater_start' : 'int8',  \n",
    "   'quater_end'   : 'int8',  \n",
    "   'month_start'  : 'int8',  \n",
    "   'month_end'    : 'int8',  \n",
    "   'year_start'   : 'int8',  \n",
    "   'year_end'     : 'int8',  \n",
    "   'group'        : 'int8',  \n",
    "   'no_events'    : 'object', \n",
    "   'holiday'      : 'object',\n",
    "    'week_number':'int8',\n",
    "       'season':'int8',\n",
    "       'quater_start':'int8',\n",
    "       'quater_end':'int8',\n",
    "       'month_start':'int8',\n",
    "       'month_end':'int8',\n",
    "       'year_start':'int8',\n",
    "       'year_end':'int8',\n",
    "       'roll_7_shift_28_mean':'float16',\n",
    "       'roll_14_shift_28_mean':'float16',\n",
    "       'roll_30_shift_28_mean':'float16',\n",
    "       'roll_60_shift_28_mean':'float16',\n",
    "       'roll_360_shift_28_mean':'float16',\n",
    "       'roll_7_shift_28_std':'float16',\n",
    "       'roll_14_shift_28_std':'float16',\n",
    "       'roll_30_shift_28_std':'float16',\n",
    "       'roll_60_shift_28_std':'float16',\n",
    "       'roll_360_shift_28_std':'float16',\n",
    "       'direct_ewm':'float16',\n",
    "       'direct_lag_28':'int16',\n",
    "       'direct_lag_35':'int16',\n",
    "       'direct_lag_42':'int16',\n",
    "       'direct_lag_49':'int16',\n",
    "       'direct_lag_56':'int16',\n",
    "       'direct_lag_63':'int16',\n",
    "       'direct_lag_70':'int16',\n",
    "       'direct_lag_77':'int16',\n",
    "       'direct_lag_84':'int16',\n",
    "       'direct_lag_91':'int16',\n",
    "       'direct_lag_98':'int16',\n",
    "       'min_price':'float16',\n",
    "       'max_price':'float16',\n",
    "       'mean_price':'float16',\n",
    "       'std_price':'float16',\n",
    "       'price_norm_1':'float16',\n",
    "       'price_norm_2':'float16',\n",
    "       'price_norm_3':'float16',\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_feather('/Users/pankaj/Predictive Analytics/Project/Intermediate Data/train1.feather')\n",
    "cv=pd.read_feather('/Users/pankaj/Predictive Analytics/Project/Intermediate Data/cv1.feather')\n",
    "test=pd.read_feather('/Users/pankaj/Predictive Analytics/Project/Intermediate Data/test1.feather')\n",
    "final_test=pd.read_feather('/Users/pankaj/Predictive Analytics/Project/Intermediate Data/final_test1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, typ in dtype.items():\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].astype(typ)\n",
    "        cv[col] = cv[col].astype(typ)\n",
    "        test[col] = test[col].astype(typ)\n",
    "        final_test[col] = final_test[col].astype(typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRMSSE Calculation\n",
    "def caluclate_WRMSSE(actual,predicted,train,weights,h,n):\n",
    "    '''This function is used to calculate RMSSE'''\n",
    "    num=((actual-predicted)**2).sum(axis=1)/h\n",
    "    denom=(train[:,1:]-train[:,:-1])**2\n",
    "    denom=denom.sum(axis=1)/(n-1)\n",
    "    return (num/denom)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances(model,train,cv,test,X_cv,X_test):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    X_cv = cv[m_lgb.feature_name()]\n",
    "    cv['pred_sales']=model.predict(X_cv)\n",
    "    df1=cv.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1886,1914)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    cv.drop(['pred_sales'],axis=1,inplace=True)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    test['pred_sales']=model.predict(X_test)\n",
    "    df1=test.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1914,1942)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances_for_store_wise_trained_model(train,cv,test,cv_pred,test_pred):\n",
    "    '''This Function is used to get WRMSSE that is used in this Case Study as a Metric For CV and Test Data where model is trained according to store id(Mainly used for Catabosst)'''\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    \n",
    "    df1=cv_pred\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    df1=test_pred\n",
    "    df1['id']=df1['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg = pd.concat([agg, temp_df[col]], ignore_index=True)\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg = pd.concat([agg, dd[col]], ignore_index=True)\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical Data's\n",
    "columns=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','wday', 'month', 'year',\n",
    "         'event_name_1','event_type_1', 'event_name_2', 'event_type_2','snap','week_number',\n",
    "         'quater_start', 'quater_end', 'month_start','month_end', 'year_start',\n",
    "         'year_end','season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=lgb.Dataset(train.drop(['id','sales','date','d'],axis=1),label=train['sales'],categorical_feature=columns,free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv=lgb.Dataset(cv.drop(['id','sales','date','d'],axis=1),label=cv['sales'],categorical_feature=columns,free_raw_data=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          \"objective\" : \"tweedie\",\n",
    "            \"metric\" :\"rmse\",\n",
    "            \"n_estimators\":6000,\n",
    "            \"force_row_wise\" : True,\n",
    "            \"learning_rate\" : 0.09,\n",
    "            \"nthread\" : 8,\n",
    "            'verbosity': 1,\n",
    "            'num_iterations' : 1000,\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'max_bin': 127,\n",
    "            'bin_construct_sample_cnt':20000000,\n",
    "            'num_leaves': 1000,\n",
    "            'min_data_in_leaf': 2**10-1,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction':0.8,\n",
    "            'bagging_fraction':0.8,\n",
    "            'bagging_freq':1,\n",
    "            'lambda_l2':0.1,\n",
    "            'boost_from_average': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total Bins 6049\n",
      "[LightGBM] [Info] Number of data points in the train set: 45174237, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.356321\n",
      "[1]\ttrain's rmse: 4.26681\tvalid's rmse: 3.53782\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\ttrain's rmse: 4.20586\tvalid's rmse: 3.47854\n",
      "[3]\ttrain's rmse: 4.14031\tvalid's rmse: 3.4161\n",
      "[4]\ttrain's rmse: 4.07148\tvalid's rmse: 3.35126\n",
      "[5]\ttrain's rmse: 4.00011\tvalid's rmse: 3.28499\n",
      "[6]\ttrain's rmse: 3.92887\tvalid's rmse: 3.21893\n",
      "[7]\ttrain's rmse: 3.85772\tvalid's rmse: 3.15325\n",
      "[8]\ttrain's rmse: 3.78531\tvalid's rmse: 3.08815\n",
      "[9]\ttrain's rmse: 3.71677\tvalid's rmse: 3.02595\n",
      "[10]\ttrain's rmse: 3.6481\tvalid's rmse: 2.96511\n",
      "[11]\ttrain's rmse: 3.58357\tvalid's rmse: 2.90822\n",
      "[12]\ttrain's rmse: 3.52003\tvalid's rmse: 2.85404\n",
      "[13]\ttrain's rmse: 3.45944\tvalid's rmse: 2.80189\n",
      "[14]\ttrain's rmse: 3.40159\tvalid's rmse: 2.75312\n",
      "[15]\ttrain's rmse: 3.34852\tvalid's rmse: 2.70759\n",
      "[16]\ttrain's rmse: 3.29863\tvalid's rmse: 2.66392\n",
      "[17]\ttrain's rmse: 3.25003\tvalid's rmse: 2.62353\n",
      "[18]\ttrain's rmse: 3.20415\tvalid's rmse: 2.58623\n",
      "[19]\ttrain's rmse: 3.16054\tvalid's rmse: 2.5516\n",
      "[20]\ttrain's rmse: 3.12012\tvalid's rmse: 2.5198\n",
      "[21]\ttrain's rmse: 3.08217\tvalid's rmse: 2.49017\n",
      "[22]\ttrain's rmse: 3.04643\tvalid's rmse: 2.46433\n",
      "[23]\ttrain's rmse: 3.01297\tvalid's rmse: 2.43985\n",
      "[24]\ttrain's rmse: 2.98346\tvalid's rmse: 2.41701\n",
      "[25]\ttrain's rmse: 2.95461\tvalid's rmse: 2.39479\n",
      "[26]\ttrain's rmse: 2.92754\tvalid's rmse: 2.37461\n",
      "[27]\ttrain's rmse: 2.90211\tvalid's rmse: 2.35635\n",
      "[28]\ttrain's rmse: 2.87774\tvalid's rmse: 2.33998\n",
      "[29]\ttrain's rmse: 2.85576\tvalid's rmse: 2.32522\n",
      "[30]\ttrain's rmse: 2.83494\tvalid's rmse: 2.31143\n",
      "[31]\ttrain's rmse: 2.8162\tvalid's rmse: 2.29875\n",
      "[32]\ttrain's rmse: 2.79764\tvalid's rmse: 2.2871\n",
      "[33]\ttrain's rmse: 2.78083\tvalid's rmse: 2.2747\n",
      "[34]\ttrain's rmse: 2.76418\tvalid's rmse: 2.26521\n",
      "[35]\ttrain's rmse: 2.75122\tvalid's rmse: 2.25668\n",
      "[36]\ttrain's rmse: 2.73682\tvalid's rmse: 2.24834\n",
      "[37]\ttrain's rmse: 2.72244\tvalid's rmse: 2.24149\n",
      "[38]\ttrain's rmse: 2.71076\tvalid's rmse: 2.23478\n",
      "[39]\ttrain's rmse: 2.69893\tvalid's rmse: 2.22869\n",
      "[40]\ttrain's rmse: 2.68855\tvalid's rmse: 2.22269\n",
      "[41]\ttrain's rmse: 2.67966\tvalid's rmse: 2.21796\n",
      "[42]\ttrain's rmse: 2.6701\tvalid's rmse: 2.21325\n",
      "[43]\ttrain's rmse: 2.66083\tvalid's rmse: 2.2084\n",
      "[44]\ttrain's rmse: 2.65355\tvalid's rmse: 2.20494\n",
      "[45]\ttrain's rmse: 2.64664\tvalid's rmse: 2.20093\n",
      "[46]\ttrain's rmse: 2.64029\tvalid's rmse: 2.19808\n",
      "[47]\ttrain's rmse: 2.63401\tvalid's rmse: 2.19523\n",
      "[48]\ttrain's rmse: 2.62732\tvalid's rmse: 2.19239\n",
      "[49]\ttrain's rmse: 2.61978\tvalid's rmse: 2.19047\n",
      "[50]\ttrain's rmse: 2.61332\tvalid's rmse: 2.18858\n",
      "[51]\ttrain's rmse: 2.60821\tvalid's rmse: 2.18644\n",
      "[52]\ttrain's rmse: 2.60361\tvalid's rmse: 2.18461\n",
      "[53]\ttrain's rmse: 2.59848\tvalid's rmse: 2.18349\n",
      "[54]\ttrain's rmse: 2.59212\tvalid's rmse: 2.18136\n",
      "[55]\ttrain's rmse: 2.58838\tvalid's rmse: 2.18057\n",
      "[56]\ttrain's rmse: 2.58434\tvalid's rmse: 2.17943\n",
      "[57]\ttrain's rmse: 2.58021\tvalid's rmse: 2.17852\n",
      "[58]\ttrain's rmse: 2.57563\tvalid's rmse: 2.17768\n",
      "[59]\ttrain's rmse: 2.57069\tvalid's rmse: 2.17721\n",
      "[60]\ttrain's rmse: 2.56606\tvalid's rmse: 2.17662\n",
      "[61]\ttrain's rmse: 2.56168\tvalid's rmse: 2.17616\n",
      "[62]\ttrain's rmse: 2.55859\tvalid's rmse: 2.17545\n",
      "[63]\ttrain's rmse: 2.55449\tvalid's rmse: 2.17443\n",
      "[64]\ttrain's rmse: 2.55137\tvalid's rmse: 2.17394\n",
      "[65]\ttrain's rmse: 2.54741\tvalid's rmse: 2.1721\n",
      "[66]\ttrain's rmse: 2.54423\tvalid's rmse: 2.17154\n",
      "[67]\ttrain's rmse: 2.54127\tvalid's rmse: 2.17129\n",
      "[68]\ttrain's rmse: 2.53871\tvalid's rmse: 2.17056\n",
      "[69]\ttrain's rmse: 2.53525\tvalid's rmse: 2.17035\n",
      "[70]\ttrain's rmse: 2.53117\tvalid's rmse: 2.17014\n",
      "[71]\ttrain's rmse: 2.52877\tvalid's rmse: 2.16972\n",
      "[72]\ttrain's rmse: 2.52658\tvalid's rmse: 2.16954\n",
      "[73]\ttrain's rmse: 2.52313\tvalid's rmse: 2.16947\n",
      "[74]\ttrain's rmse: 2.52042\tvalid's rmse: 2.16931\n",
      "[75]\ttrain's rmse: 2.51782\tvalid's rmse: 2.16913\n",
      "[76]\ttrain's rmse: 2.51532\tvalid's rmse: 2.16855\n",
      "[77]\ttrain's rmse: 2.51295\tvalid's rmse: 2.16878\n",
      "[78]\ttrain's rmse: 2.51057\tvalid's rmse: 2.16815\n",
      "[79]\ttrain's rmse: 2.50805\tvalid's rmse: 2.16779\n",
      "[80]\ttrain's rmse: 2.50599\tvalid's rmse: 2.1677\n",
      "[81]\ttrain's rmse: 2.5041\tvalid's rmse: 2.1673\n",
      "[82]\ttrain's rmse: 2.50167\tvalid's rmse: 2.16716\n",
      "[83]\ttrain's rmse: 2.50004\tvalid's rmse: 2.16735\n",
      "[84]\ttrain's rmse: 2.49831\tvalid's rmse: 2.16724\n",
      "[85]\ttrain's rmse: 2.49593\tvalid's rmse: 2.1672\n",
      "[86]\ttrain's rmse: 2.49339\tvalid's rmse: 2.16766\n",
      "[87]\ttrain's rmse: 2.49119\tvalid's rmse: 2.16729\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttrain's rmse: 2.50167\tvalid's rmse: 2.16716\n"
     ]
    }
   ],
   "source": [
    "m_lgb = lgb.train(\n",
    "    params,\n",
    "    data,\n",
    "    valid_sets=[data, data_cv],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=5),\n",
    "        lgb.log_evaluation(period=1)  # same as verbose_eval=1\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x176d2f340>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lgb.save_model('lgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_performances(m_lgb,train,cv,test,cv.drop(['id','sales','date','d','prices'],axis=1),test.drop(['id','sales','date','d'],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_sales']=m_lgb.predict(test.drop(['id','sales','date','d','prices', 'pred_sales'],axis=1))\n",
    "final_test['pred_sales']=m_lgb.predict(final_test.drop(['id','sales','date','d',],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=test.pivot_table(index='id',columns='date',values='pred_sales')\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df['id']=df['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "dic={}\n",
    "l=test['date'].unique()\n",
    "for i,day in enumerate(l):\n",
    "    dic[day]='F'+str(i+1)\n",
    "df.rename(columns=dic,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=final_test.pivot_table(index='id',columns='d',values='pred_sales')\n",
    "df1.reset_index(level=0,inplace=True)\n",
    "dic={}\n",
    "for i,day in enumerate(range(1942,1970)):\n",
    "    dic['d_'+str(day)]='F'+str(i+1)\n",
    "df1.rename(columns=dic,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305854,
     "sourceId": 11653920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
